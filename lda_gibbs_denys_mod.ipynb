{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iron/Repositories/classes/ml_2_classes/ML2_Exercises/pgm/ex7/denys/LDA_nips_papers/.env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.notebook import trange\n",
    "import contextlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import edward2 as ed\n",
    "import tensorflow as tf\n",
    "from scipy.special import digamma\n",
    "from pickle import dump, load\n",
    "from scipy.sparse import csr_matrix\n",
    "import tensorflow_probability as tfp\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing import strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
    "from gensim.parsing import preprocess_string\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# tensorflow does not work with new numpy versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/iron/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/iron/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/iron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc = pd.read_pickle('data_proc.pkl')\n",
    "# data_enc = pd.read_pickle('data.pkl')\n",
    "# with open('word_to_idx.pkl', 'rb') as f:\n",
    "#     words_to_idx = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4000\n",
    "\n",
    "seed = 42\n",
    "data_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.Series(data_train.data).sample(n).copy()\n",
    "# data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stem_filters = [strip_tags,\n",
    "                        strip_numeric,\n",
    "                        strip_punctuation, \n",
    "                        lambda x: x.lower(),\n",
    "                        lambda s: re.sub(r'\\b\\w{1,2}\\b', ' ', s),\n",
    "                        strip_multiple_whitespaces,\n",
    "                        remove_stopwords\n",
    "                     ]\n",
    "\n",
    "def text_processing(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(i) for i in preprocess_string(document, clean_stem_filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_func(data):\n",
    "    with tqdm_joblib(tqdm(desc=\"Preprocessing\", total=len(data))) as progress_bar:\n",
    "        data_proc = Parallel(n_jobs=1)(delayed(text_processing)(text) for text in data)\n",
    "        data_proc = pd.Series(data_proc, index=data.index, name='data')\n",
    "    return data_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc = proc_func(data)\n",
    "# data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preproc_func(text, stop_words):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'(\\d+)', '', text)\n",
    "#     text = re.sub(r'(\\n)|(\\t)', ' ', text)\n",
    "#     text = text.translate({ ord(c): None for c in string.punctuation })\n",
    "#     text = text.strip()\n",
    "    \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = word_tokenize(text)\n",
    "#     text = [i for i in tokens if not i in stop_words and len(i) > 1]\n",
    "#     text = [lemmatizer.lemmatize(word) for word in text]\n",
    "#     return text\n",
    "\n",
    "# with tqdm_joblib(tqdm(desc=\"Preprocessing\", total=len(data_n))) as progress_bar:\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     data_proc = Parallel(n_jobs=1)(delayed(preproc_func)(text, stop_words) for text in data_n['paper_text'])\n",
    "#     data_proc = pd.Series(data_proc, index=data_n.index, name='paper_text')\n",
    "# data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode2(text, word_dict):\n",
    "    return np.asarray(word_dict.doc2idx(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_dict = Dictionary(data_proc)\n",
    "# data_enc = data_proc.progress_apply(lambda x: encode2(x, word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc.to_pickle('data_proc.pkl')\n",
    "# data_enc.to_pickle('data.pkl')\n",
    "\n",
    "# with open('word_to_idx.pkl', 'wb') as f:\n",
    "#     dump(words_to_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 10\n",
    "# D = len(data_enc)\n",
    "# Ns = data_enc.apply(lambda x: len(x)).to_numpy().astype(int)\n",
    "# N = Ns.sum()\n",
    "# V = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(K, D, Ns, N, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indices(data, D, K, V):\n",
    "    Ns = np.empty(D, dtype=int)\n",
    "     \n",
    "    for i, doc in enumerate(data):\n",
    "        Ns[i] = len(doc)\n",
    "    \n",
    "    N = Ns.sum()\n",
    "    rows = np.empty(N, dtype=np.int64)\n",
    "    cols = np.empty(N, dtype=np.int64)\n",
    "    v_cols = np.empty(N, dtype=np.int64)\n",
    "    \n",
    "    last_idx = 0\n",
    "    \n",
    "    for i, doc in tqdm(enumerate(data), total=D):\n",
    "        n = len(doc)\n",
    "        rows[last_idx:last_idx+n] = i\n",
    "        cols[last_idx:last_idx+n] = np.arange(n, dtype=np.int64)\n",
    "        v_cols[last_idx:last_idx+n] = doc\n",
    "        last_idx += n\n",
    "        \n",
    "    K_idx = np.tile(np.arange(K), N)\n",
    "    left_indices = np.stack((K_idx, np.repeat(v_cols, K)), axis=1)\n",
    "    dt_indices = np.stack((np.repeat(rows, K), K_idx), axis=1)\n",
    "    \n",
    "    return left_indices, dt_indices, N, Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not optimize much, as we use it only once\n",
    "@tf.function\n",
    "def init_matrices_internal(data_ragged, alpha, beta, D, N, K, V, Ns):\n",
    "    dt = tf.zeros((D, K), dtype=np.int64)\n",
    "    wt = tf.zeros((K, V), dtype=np.int64)  \n",
    "    \n",
    "    alpha_d = tf.repeat(tf.expand_dims(alpha, 0), N, axis=0)\n",
    "    tdp = ed.Dirichlet(alpha_d)\n",
    "    dw = ed.Categorical(probs=tdp)\n",
    "    dw_r = tf.RaggedTensor.from_row_lengths(dw, Ns)\n",
    "\n",
    "    upd_cond = lambda i, dt, wt: i < D\n",
    "    def upd_body(i, dt, wt):\n",
    "        td = dw_r[i]\n",
    "        y, idx, counts = tf.unique_with_counts(td, out_idx=tf.int64)\n",
    "\n",
    "        didx = tf.stack((tf.fill((tf.size(y),), i), y), axis=1)\n",
    "\n",
    "        dt = tf.tensor_scatter_nd_update(dt, didx, counts)\n",
    "        \n",
    "        tidx = tf.stack((tf.cast(td, dtype=tf.int64), data_ragged[i]), axis=1) \n",
    "        wt = tf.tensor_scatter_nd_add(wt, tidx, tf.ones(tf.size(data_ragged[i]), dtype=np.int64))\n",
    "        return i + 1, dt, wt\n",
    "\n",
    "    j = tf.constant(0)\n",
    "    j, dt, wt = tf.while_loop(upd_cond, upd_body, [j, dt, wt])\n",
    "    \n",
    "    dw = tf.concat(dw, 0)\n",
    "    pz = tf.random.uniform((N, K)) + 1e-5\n",
    "    pz /= tf.math.reduce_sum(pz, axis=1, keepdims=True)\n",
    "         \n",
    "    return dt, wt, dw, pz, data_ragged\n",
    "\n",
    "\n",
    "def init_matrices(data, alpha, beta, D, N, K, V, Ns):\n",
    "    data_ragged = tf.ragged.stack([tf.convert_to_tensor(doc, dtype=tf.int64) for doc in data])\n",
    "    return init_matrices_internal(data_ragged, alpha, beta, D, N, K, V, Ns) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = tf.zeros((K,)) + 0.5 #tf.random.uniform((K,))\n",
    "# beta = tf.zeros(()) + 0.5 #tf.random.uniform(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log_likelihood(beta, wt, K, V):\n",
    "    wt = tf.cast(wt, tf.float32)\n",
    "    return -K * tf.math.lbeta(tf.repeat(beta[None], V)) + tf.math.reduce_sum(tf.math.lbeta(wt + beta))\n",
    "\n",
    "@tf.function\n",
    "def train_internal(alpha, beta, D, N, K, V, Ns, dt, wt, dw, pz, data_ragged, left_indices, dt_indices, max_it, parallel_iterations, rtol, atol):\n",
    "#     train_cond = lambda i, dt, wt, dw, pz, ll_old, ll: tf.logical_and(i < max_it, \n",
    "#                                                           tf.logical_or(i == 0, \n",
    "#                                                                         tf.logical_not(tf.experimental.numpy.allclose(ll_old, ll, rtol=rtol, atol=atol))))\n",
    "    train_cond = lambda i, dt, wt, dw, pz, ll_old, ll: i < max_it\n",
    "    \n",
    "    def train_body(i, dt, wt, dw, pz, ll_old, ll):   \n",
    "        ll_old = tf.identity(ll)\n",
    "        dt_float = tf.cast(dt, tf.float32)\n",
    "        wt_float = tf.cast(wt, tf.float32)\n",
    "\n",
    "        term = (wt_float + beta) / (tf.math.reduce_sum(wt_float, axis=1, keepdims=True) + V * beta)\n",
    "        left = tf.expand_dims(alpha, 1) * term\n",
    "        left = tf.gather_nd(left, left_indices)\n",
    "        right = tf.gather_nd(dt_float, dt_indices) * tf.gather_nd(term, left_indices)\n",
    "\n",
    "        pz_new = left + right\n",
    "        pz_new = tf.reshape(pz_new, (N, K))\n",
    "\n",
    "        dw_new = ed.Categorical(probs=pz_new, dtype=tf.int32)\n",
    "        dw_new_idx = tf.stack((tf.cast(tf.range(0, N), dtype=tf.int32), dw_new.value), axis=1)\n",
    "        \n",
    "        # MH step\n",
    "        pz_new_masked = tf.gather_nd(pz_new, dw_new_idx)\n",
    "        pz_masked = tf.gather_nd(pz, dw_new_idx)\n",
    "        ratios = pz_new_masked / pz_masked\n",
    "\n",
    "        u = tf.random.uniform(ratios.shape)\n",
    "        mask = tf.math.less_equal(u, ratios)\n",
    "        indices_upd = tf.where(mask)\n",
    "        vals_upd = tf.boolean_mask(dw_new, mask)\n",
    "        dw = tf.tensor_scatter_nd_update(dw, indices_upd, vals_upd)\n",
    "        dw_r = tf.RaggedTensor.from_row_lengths(dw, Ns)\n",
    "        dt = tf.zeros_like(dt)\n",
    "        wt = tf.zeros_like(wt)\n",
    "\n",
    "        upd_cond = lambda i, dt, wt: i < D\n",
    "        def upd_body(i, dt, wt):\n",
    "            td = dw_r[i]\n",
    "            y, idx, counts = tf.unique_with_counts(td, out_idx=tf.int64)\n",
    "\n",
    "            didx = tf.stack((tf.fill((tf.size(y),), i), y), axis=1)\n",
    "            dt = tf.tensor_scatter_nd_update(dt, didx, counts)\n",
    "\n",
    "            tidx = tf.stack((tf.cast(td, dtype=tf.int64), data_ragged[i]), axis=1) \n",
    "            wt = tf.tensor_scatter_nd_add(wt, tidx, tf.ones(tf.size(data_ragged[i]), dtype=np.int64))\n",
    "            \n",
    "            return i + 1, dt, wt\n",
    "\n",
    "        j = tf.constant(0)\n",
    "        j, dt, wt = tf.while_loop(upd_cond, upd_body, [j, dt, wt], parallel_iterations=parallel_iterations)\n",
    "        ll = log_likelihood(beta, wt, K, V)\n",
    "        tf.print('Epoch:', i, 'LL =', ll)            \n",
    "    \n",
    "        return i + 1, dt, wt, dw, pz, ll_old, ll\n",
    "\n",
    "    i = tf.constant(0)\n",
    "    ll_old = tf.constant(-np.inf, dtype=tf.float32)\n",
    "    ll = tf.constant(-np.inf, dtype=tf.float32)\n",
    "    i, dt, wt, dw, pz, _, ll = tf.while_loop(train_cond, train_body, [i, dt, wt, dw, pz, ll_old, ll], parallel_iterations=parallel_iterations)\n",
    "    tf.print('Converged in', i,  'iterations')\n",
    "    return dt, wt, ll\n",
    "\n",
    "def train(data, alpha, beta, D, K, V, max_it=1000, parallel_iterations=1, rtol=1e-4, atol=1e-3):\n",
    "    start = time.time()\n",
    "    tf.print('Preparing...')\n",
    "    left_indices, dt_indices, N, Ns = create_indices(data_enc, D, K, V)\n",
    "    dt, wt, dw, pz, data_ragged = init_matrices(data, alpha, beta, D, N, K, V, Ns)\n",
    "    \n",
    "    dt, wt, ll = train_internal(alpha, beta, D, N, K, V, Ns, dt, wt, dw, pz, data_ragged, \n",
    "                                left_indices, dt_indices, max_it, parallel_iterations, rtol, atol)\n",
    "    end = time.time()\n",
    "    print('Time:', end-start, 's')\n",
    "    return dt, wt, ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt, wt, ll = train(data_enc, alpha, beta, D, K, V, max_it=250, parallel_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_phi_matrix(wt, beta, V):\n",
    "    wt = tf.cast(wt, tf.float32)\n",
    "    phi = (wt + beta) / (tf.math.reduce_sum(wt, axis=1, keepdims=True) + V * beta)\n",
    "    return phi\n",
    "\n",
    "@tf.function   \n",
    "def get_theta_matrix(dt, alpha, K):\n",
    "    dt = tf.cast(dt, tf.float32)\n",
    "    alpha = tf.expand_dims(alpha, 0)\n",
    "    theta = (dt + alpha) / (tf.math.reduce_sum(dt, axis=1, keepdims=True) + K * alpha)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = get_phi_matrix(wt, beta, V)\n",
    "# theta = get_theta_matrix(dt, alpha, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h in range(K):\n",
    "#     print(([word_dict[i] for i in tf.math.top_k(phi[h], k=10)[1].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate doc\n",
    "# theta_d = ed.Dirichlet(alpha)\n",
    "# for i in range(100):\n",
    "#     z_dn = ed.Categorical(probs=theta_d)\n",
    "#     w_dn = ed.Categorical(probs=beta[z_dn])\n",
    "#     print(word_dict[w_dn.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic top words\n",
    "def get_topics(phi, word_dict, k):\n",
    "    topics = []\n",
    "    weights = []\n",
    "    for h in range(k):\n",
    "        top_phi = tf.math.top_k(phi[h], k=10)\n",
    "        topic = [word_dict[i] for i in top_phi[1].numpy()] \n",
    "        weight = list(top_phi[0].numpy())\n",
    "    #     print(([idx_to_word[i] for i in tf.math.top_k(beta[h], k=10)[1].numpy()]))\n",
    "        topics.append(topic)\n",
    "        weights.append(weight)\n",
    "\n",
    "    return (np.asarray(topics), np.asarray(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics, weights = get_topics(phi, word_dict, K)\n",
    "# topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence(topics, text_data, word_dict, coherence_type='c_v'):\n",
    "    coherence_model_lda = CoherenceModel(topics=topics, texts=text_data, dictionary=word_dict, coherence='c_v')\n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence = get_coherence(topics, data_proc, word_dict, 'c_v')\n",
    "# coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716137ac96e44735b49e988192a119a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa280b155124c57a7d6b2a43832aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea925e417ac340609b49e07eaeeb273a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b75bfab74d54da58560efb7c34aeb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4 0.1 0.1 46116\n",
      "Preparing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80644b8d492437eae48e3686d51c7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LL = -5229215\n",
      "Epoch: 1 LL = -5224879.5\n",
      "Epoch: 2 LL = -5220481.5\n",
      "Epoch: 3 LL = -5216161\n",
      "Epoch: 4 LL = -5211152.5\n",
      "Epoch: 5 LL = -5.20592e+06\n",
      "Epoch: 6 LL = -5200168.5\n",
      "Epoch: 7 LL = -5193778\n",
      "Epoch: 8 LL = -5.18704e+06\n",
      "Epoch: 9 LL = -5179463\n",
      "Epoch: 10 LL = -5170823\n",
      "Epoch: 11 LL = -5161517.5\n",
      "Epoch: 12 LL = -5150920.5\n",
      "Epoch: 13 LL = -5139619.5\n",
      "Epoch: 14 LL = -5127213\n",
      "Epoch: 15 LL = -5114859.5\n",
      "Epoch: 16 LL = -5.10208e+06\n",
      "Epoch: 17 LL = -5089343.5\n",
      "Epoch: 18 LL = -5076419.5\n",
      "Epoch: 19 LL = -5063556.5\n",
      "Epoch: 20 LL = -5051467.5\n",
      "Epoch: 21 LL = -5039575.5\n",
      "Epoch: 22 LL = -5028834\n",
      "Epoch: 23 LL = -5018845\n",
      "Epoch: 24 LL = -5009054.5\n",
      "Epoch: 25 LL = -5000524.5\n",
      "Epoch: 26 LL = -4992374.5\n",
      "Epoch: 27 LL = -4984277.5\n",
      "Epoch: 28 LL = -4976119.5\n",
      "Epoch: 29 LL = -4968167.5\n",
      "Epoch: 30 LL = -4960701.5\n",
      "Epoch: 31 LL = -4952967.5\n",
      "Epoch: 32 LL = -4945105.5\n",
      "Epoch: 33 LL = -4937824\n",
      "Epoch: 34 LL = -4929927.5\n",
      "Epoch: 35 LL = -4922958.5\n",
      "Epoch: 36 LL = -4916349\n",
      "Epoch: 37 LL = -4909481\n",
      "Epoch: 38 LL = -4903043.5\n",
      "Epoch: 39 LL = -4896837.5\n",
      "Epoch: 40 LL = -4891038\n",
      "Epoch: 41 LL = -4885150.5\n",
      "Epoch: 42 LL = -4879176.5\n",
      "Epoch: 43 LL = -4873202\n",
      "Epoch: 44 LL = -4867710.5\n",
      "Epoch: 45 LL = -4862408.5\n",
      "Epoch: 46 LL = -4856903.5\n",
      "Epoch: 47 LL = -4851260.5\n",
      "Epoch: 48 LL = -4845881.5\n",
      "Epoch: 49 LL = -4840755.5\n",
      "Epoch: 50 LL = -4835980.5\n",
      "Epoch: 51 LL = -4831175.5\n",
      "Epoch: 52 LL = -4826824.5\n",
      "Epoch: 53 LL = -4822375\n",
      "Epoch: 54 LL = -4817861.5\n",
      "Epoch: 55 LL = -4813982.5\n",
      "Epoch: 56 LL = -4810025.5\n",
      "Epoch: 57 LL = -4806112\n",
      "Epoch: 58 LL = -4802191\n",
      "Epoch: 59 LL = -4798600.5\n",
      "Epoch: 60 LL = -4794818.5\n",
      "Epoch: 61 LL = -4791151\n",
      "Epoch: 62 LL = -4787500.5\n",
      "Epoch: 63 LL = -4783849.5\n",
      "Epoch: 64 LL = -4780448.5\n",
      "Epoch: 65 LL = -4777010.5\n",
      "Epoch: 66 LL = -4774026.5\n",
      "Epoch: 67 LL = -4770649.5\n",
      "Epoch: 68 LL = -4767388.5\n",
      "Epoch: 69 LL = -4764417\n",
      "Epoch: 70 LL = -4761521.5\n",
      "Epoch: 71 LL = -4758293.5\n",
      "Epoch: 72 LL = -4755238.5\n",
      "Epoch: 73 LL = -4752593.5\n",
      "Epoch: 74 LL = -4749807.5\n",
      "Epoch: 75 LL = -4747232.5\n",
      "Epoch: 76 LL = -4744327.5\n",
      "Epoch: 77 LL = -4741527.5\n",
      "Epoch: 78 LL = -4738699.5\n",
      "Epoch: 79 LL = -4735959.5\n",
      "Epoch: 80 LL = -4733382\n",
      "Epoch: 81 LL = -4730780.5\n",
      "Epoch: 82 LL = -4728253.5\n",
      "Epoch: 83 LL = -4725886.5\n",
      "Epoch: 84 LL = -4723396.5\n",
      "Epoch: 85 LL = -4720992\n",
      "Epoch: 86 LL = -4718505.5\n",
      "Epoch: 87 LL = -4716431.5\n",
      "Epoch: 88 LL = -4714042.5\n",
      "Epoch: 89 LL = -4711784.5\n",
      "Epoch: 90 LL = -4709723.5\n",
      "Epoch: 91 LL = -4707895\n",
      "Epoch: 92 LL = -4705801\n",
      "Epoch: 93 LL = -4703797.5\n",
      "Epoch: 94 LL = -4701781\n",
      "Epoch: 95 LL = -4699565.5\n",
      "Epoch: 96 LL = -4697693.5\n",
      "Epoch: 97 LL = -4695594.5\n",
      "Epoch: 98 LL = -4693596.5\n",
      "Epoch: 99 LL = -4691695.5\n",
      "Epoch: 100 LL = -4689907.5\n",
      "Epoch: 101 LL = -4687889.5\n",
      "Epoch: 102 LL = -4685759.5\n",
      "Epoch: 103 LL = -4683975.5\n",
      "Epoch: 104 LL = -4682160.5\n",
      "Epoch: 105 LL = -4680402.5\n",
      "Epoch: 106 LL = -4678841.5\n",
      "Epoch: 107 LL = -4676911.5\n",
      "Epoch: 108 LL = -4675089.5\n",
      "Epoch: 109 LL = -4673321\n",
      "Epoch: 110 LL = -4671567.5\n",
      "Epoch: 111 LL = -4669941.5\n",
      "Epoch: 112 LL = -4668315\n",
      "Epoch: 113 LL = -4666794.5\n",
      "Epoch: 114 LL = -4665187\n",
      "Epoch: 115 LL = -4663755\n",
      "Epoch: 116 LL = -4662212\n",
      "Epoch: 117 LL = -4.66074e+06\n",
      "Epoch: 118 LL = -4659105.5\n",
      "Epoch: 119 LL = -4657676.5\n",
      "Epoch: 120 LL = -4656369.5\n",
      "Epoch: 121 LL = -4654990.5\n",
      "Epoch: 122 LL = -4653525.5\n",
      "Epoch: 123 LL = -4652099.5\n",
      "Epoch: 124 LL = -4650885\n",
      "Epoch: 125 LL = -4649471.5\n",
      "Epoch: 126 LL = -4648270.5\n",
      "Epoch: 127 LL = -4.64691e+06\n",
      "Epoch: 128 LL = -4645679.5\n",
      "Epoch: 129 LL = -4644238\n",
      "Epoch: 130 LL = -4642853.5\n",
      "Epoch: 131 LL = -4641598.5\n",
      "Epoch: 132 LL = -4640419\n",
      "Epoch: 133 LL = -4639077\n",
      "Epoch: 134 LL = -4637894\n",
      "Epoch: 135 LL = -4636616\n",
      "Epoch: 136 LL = -4635405.5\n",
      "Epoch: 137 LL = -4634176.5\n",
      "Epoch: 138 LL = -4633015\n",
      "Epoch: 139 LL = -4631998.5\n",
      "Epoch: 140 LL = -4630807\n",
      "Epoch: 141 LL = -4629779.5\n",
      "Epoch: 142 LL = -4628492.5\n",
      "Epoch: 143 LL = -4.62722e+06\n",
      "Epoch: 144 LL = -4626285.5\n",
      "Epoch: 145 LL = -4625248.5\n",
      "Epoch: 146 LL = -4.6242e+06\n",
      "Epoch: 147 LL = -4623177\n",
      "Epoch: 148 LL = -4622237\n",
      "Epoch: 149 LL = -4621156\n",
      "Epoch: 150 LL = -4620131\n",
      "Epoch: 151 LL = -4618934.5\n",
      "Epoch: 152 LL = -4618071.5\n",
      "Epoch: 153 LL = -4616946\n",
      "Epoch: 154 LL = -4615963.5\n",
      "Epoch: 155 LL = -4615043\n",
      "Epoch: 156 LL = -4.61405e+06\n",
      "Epoch: 157 LL = -4.61314e+06\n",
      "Epoch: 158 LL = -4612245.5\n",
      "Epoch: 159 LL = -4611394.5\n",
      "Epoch: 160 LL = -4610653.5\n",
      "Epoch: 161 LL = -4.60978e+06\n",
      "Epoch: 162 LL = -4.60869e+06\n",
      "Epoch: 163 LL = -4607858\n",
      "Epoch: 164 LL = -4607051\n",
      "Epoch: 165 LL = -4606131.5\n",
      "Epoch: 166 LL = -4605289.5\n",
      "Epoch: 167 LL = -4604433.5\n",
      "Epoch: 168 LL = -4603512.5\n",
      "Epoch: 169 LL = -4602746\n",
      "Epoch: 170 LL = -4601883.5\n",
      "Epoch: 171 LL = -4601227.5\n",
      "Epoch: 172 LL = -4600515\n",
      "Epoch: 173 LL = -4599744.5\n",
      "Epoch: 174 LL = -4599065\n",
      "Epoch: 175 LL = -4598309.5\n",
      "Epoch: 176 LL = -4597491.5\n",
      "Epoch: 177 LL = -4596766\n",
      "Epoch: 178 LL = -4596076.5\n",
      "Epoch: 179 LL = -4595324.5\n",
      "Epoch: 180 LL = -4594579.5\n",
      "Epoch: 181 LL = -4593806.5\n",
      "Epoch: 182 LL = -4593021\n",
      "Epoch: 183 LL = -4592349.5\n",
      "Epoch: 184 LL = -4591631\n",
      "Epoch: 185 LL = -4590794.5\n",
      "Epoch: 186 LL = -4590072\n",
      "Epoch: 187 LL = -4589451\n",
      "Epoch: 188 LL = -4588705\n",
      "Epoch: 189 LL = -4588134\n",
      "Epoch: 190 LL = -4587342.5\n",
      "Epoch: 191 LL = -4586578.5\n",
      "Epoch: 192 LL = -4585842\n",
      "Epoch: 193 LL = -4585217.5\n",
      "Epoch: 194 LL = -4584476.5\n",
      "Epoch: 195 LL = -4583762.5\n",
      "Epoch: 196 LL = -4583068\n",
      "Epoch: 197 LL = -4582422\n",
      "Epoch: 198 LL = -4581911.5\n",
      "Epoch: 199 LL = -4581318\n",
      "Epoch: 200 LL = -4580638\n",
      "Epoch: 201 LL = -4579977.5\n",
      "Epoch: 202 LL = -4579380.5\n",
      "Epoch: 203 LL = -4.57886e+06\n",
      "Epoch: 204 LL = -4578302.5\n",
      "Epoch: 205 LL = -4577698.5\n",
      "Epoch: 206 LL = -4.57689e+06\n",
      "Epoch: 207 LL = -4576297\n",
      "Epoch: 208 LL = -4575644\n",
      "Epoch: 209 LL = -4575044.5\n",
      "Epoch: 210 LL = -4574278.5\n",
      "Epoch: 211 LL = -4573739\n",
      "Epoch: 212 LL = -4573198.5\n",
      "Epoch: 213 LL = -4572673.5\n",
      "Epoch: 214 LL = -4572177\n",
      "Epoch: 215 LL = -4571538.5\n",
      "Epoch: 216 LL = -4571012.5\n",
      "Epoch: 217 LL = -4570490.5\n",
      "Epoch: 218 LL = -4570039\n",
      "Epoch: 219 LL = -4569517\n",
      "Epoch: 220 LL = -4568977.5\n",
      "Epoch: 221 LL = -4568431.5\n",
      "Epoch: 222 LL = -4567757.5\n",
      "Epoch: 223 LL = -4567441.5\n",
      "Epoch: 224 LL = -4566928.5\n",
      "Epoch: 225 LL = -4566353.5\n",
      "Epoch: 226 LL = -4565823.5\n",
      "Epoch: 227 LL = -4565412.5\n",
      "Epoch: 228 LL = -4564943\n",
      "Epoch: 229 LL = -4564333.5\n",
      "Epoch: 230 LL = -4563811\n",
      "Epoch: 231 LL = -4563325.5\n",
      "Epoch: 232 LL = -4562802.5\n",
      "Epoch: 233 LL = -4562404\n",
      "Epoch: 234 LL = -4561764.5\n",
      "Epoch: 235 LL = -4561252\n",
      "Epoch: 236 LL = -4560709\n",
      "Epoch: 237 LL = -4560326.5\n",
      "Epoch: 238 LL = -4559924.5\n",
      "Epoch: 239 LL = -4559423\n",
      "Epoch: 240 LL = -4558935.5\n",
      "Epoch: 241 LL = -4558484.5\n",
      "Epoch: 242 LL = -4557875.5\n",
      "Epoch: 243 LL = -4.55737e+06\n",
      "Epoch: 244 LL = -4556946\n",
      "Epoch: 245 LL = -4556506.5\n",
      "Epoch: 246 LL = -4556010.5\n",
      "Epoch: 247 LL = -4555616.5\n",
      "Epoch: 248 LL = -4555162\n",
      "Epoch: 249 LL = -4554785.5\n",
      "Converged in 250 iterations\n",
      "Time: 458.0178928375244 s\n",
      "Perplexity: 2716.7039 Coherence: 0.46783834784262146\n",
      "4000 4 0.5 0.5 46116\n",
      "Preparing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fdaa07ec4c41bbb9089a4cf7c56e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LL = -5229123.5\n",
      "Epoch: 1 LL = -5225571.5\n",
      "Epoch: 2 LL = -5.22203e+06\n",
      "Epoch: 3 LL = -5218353.5\n",
      "Epoch: 4 LL = -5214522\n",
      "Epoch: 5 LL = -5210314.5\n",
      "Epoch: 6 LL = -5205636.5\n",
      "Epoch: 7 LL = -5200443.5\n",
      "Epoch: 8 LL = -5194699\n",
      "Epoch: 9 LL = -5188539.5\n",
      "Epoch: 10 LL = -5181951\n",
      "Epoch: 11 LL = -5174558.5\n",
      "Epoch: 12 LL = -5166658\n",
      "Epoch: 13 LL = -5157575\n",
      "Epoch: 14 LL = -5147818\n",
      "Epoch: 15 LL = -5137938.5\n",
      "Epoch: 16 LL = -5127283.5\n",
      "Epoch: 17 LL = -5116609\n",
      "Epoch: 18 LL = -5106463.5\n",
      "Epoch: 19 LL = -5096279.5\n",
      "Epoch: 20 LL = -5086691\n",
      "Epoch: 21 LL = -5077648.5\n",
      "Epoch: 22 LL = -5068381\n",
      "Epoch: 23 LL = -5058899\n",
      "Epoch: 24 LL = -5049604.5\n",
      "Epoch: 25 LL = -5041427\n",
      "Epoch: 26 LL = -5033509.5\n",
      "Epoch: 27 LL = -5025808\n",
      "Epoch: 28 LL = -5018303\n",
      "Epoch: 29 LL = -5011371.5\n",
      "Epoch: 30 LL = -5004355.5\n",
      "Epoch: 31 LL = -4997743.5\n",
      "Epoch: 32 LL = -4.99108e+06\n",
      "Epoch: 33 LL = -4984588.5\n",
      "Epoch: 34 LL = -4978389\n",
      "Epoch: 35 LL = -4972300.5\n",
      "Epoch: 36 LL = -4966532.5\n",
      "Epoch: 37 LL = -4960862.5\n",
      "Epoch: 38 LL = -4954885\n",
      "Epoch: 39 LL = -4949141.5\n",
      "Epoch: 40 LL = -4943749\n",
      "Epoch: 41 LL = -4938521.5\n",
      "Epoch: 42 LL = -4933339\n",
      "Epoch: 43 LL = -4928544.5\n",
      "Epoch: 44 LL = -4923694.5\n",
      "Epoch: 45 LL = -4918963.5\n",
      "Epoch: 46 LL = -4914272.5\n",
      "Epoch: 47 LL = -4909554.5\n",
      "Epoch: 48 LL = -4905181.5\n",
      "Epoch: 49 LL = -4901019\n",
      "Epoch: 50 LL = -4896613.5\n",
      "Epoch: 51 LL = -4892352.5\n",
      "Epoch: 52 LL = -4888272.5\n",
      "Epoch: 53 LL = -4884341\n",
      "Epoch: 54 LL = -4880718.5\n",
      "Epoch: 55 LL = -4877388\n",
      "Epoch: 56 LL = -4874056.5\n",
      "Epoch: 57 LL = -4870601.5\n",
      "Epoch: 58 LL = -4867346\n",
      "Epoch: 59 LL = -4864306.5\n",
      "Epoch: 60 LL = -4861318.5\n",
      "Epoch: 61 LL = -4857998\n",
      "Epoch: 62 LL = -4855108.5\n",
      "Epoch: 63 LL = -4852102.5\n",
      "Epoch: 64 LL = -4849252.5\n",
      "Epoch: 65 LL = -4846304.5\n",
      "Epoch: 66 LL = -4843317.5\n",
      "Epoch: 67 LL = -4840470.5\n",
      "Epoch: 68 LL = -4837558.5\n",
      "Epoch: 69 LL = -4835085.5\n",
      "Epoch: 70 LL = -4832590.5\n",
      "Epoch: 71 LL = -4829747.5\n",
      "Epoch: 72 LL = -4827284.5\n",
      "Epoch: 73 LL = -4824807\n",
      "Epoch: 74 LL = -4822305.5\n",
      "Epoch: 75 LL = -4819974.5\n",
      "Epoch: 76 LL = -4817865\n",
      "Epoch: 77 LL = -4815717.5\n",
      "Epoch: 78 LL = -4813296.5\n",
      "Epoch: 79 LL = -4811082.5\n",
      "Epoch: 80 LL = -4808755.5\n",
      "Epoch: 81 LL = -4.80668e+06\n",
      "Epoch: 82 LL = -4804552\n",
      "Epoch: 83 LL = -4802512\n",
      "Epoch: 84 LL = -4800179.5\n",
      "Epoch: 85 LL = -4797905.5\n",
      "Epoch: 86 LL = -4795756.5\n",
      "Epoch: 87 LL = -4793933\n",
      "Epoch: 88 LL = -4791864.5\n",
      "Epoch: 89 LL = -4789865.5\n",
      "Epoch: 90 LL = -4787988\n",
      "Epoch: 91 LL = -4786144.5\n",
      "Epoch: 92 LL = -4784051.5\n",
      "Epoch: 93 LL = -4782243.5\n",
      "Epoch: 94 LL = -4.78054e+06\n",
      "Epoch: 95 LL = -4778689.5\n",
      "Epoch: 96 LL = -4777114\n",
      "Epoch: 97 LL = -4.77542e+06\n",
      "Epoch: 98 LL = -4.77406e+06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Ns = [4000, 2000, 1000]\n",
    "Ks = range(4, 22, 3)\n",
    "alphas = [0.1, 0.5]\n",
    "betas = [0.1, 0.5]\n",
    "\n",
    "params = []\n",
    "for n in Ns:\n",
    "    n_params = []\n",
    "    for k in Ks:\n",
    "        for alpha, beta in zip(alphas,betas):\n",
    "            n_params.append((k, alpha, beta))\n",
    "    params.append((n, n_params))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for n, n_params in tqdm(params):\n",
    "    data_n = pd.Series(data_train.data).sample(n).copy()\n",
    "    data_proc = proc_func(data_n)\n",
    "    word_dict = Dictionary(data_proc)\n",
    "    data_enc = data_proc.progress_apply(lambda x: encode2(x, word_dict))\n",
    "    D = len(data_enc)\n",
    "    Ns = data_enc.apply(lambda x: len(x)).to_numpy().astype(int)\n",
    "    N = Ns.sum()\n",
    "    V = len(word_dict)\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"N\", \"K\", \"alpha\", \"beta\", \"ll\", \"perplexity\", \"coherence\", \"topics\", \"weights\"])\n",
    "    for k, alpha, beta in tqdm(n_params):\n",
    "        np.random.seed(42)\n",
    "        print(n, k, alpha, beta, V)\n",
    "        alpha_ = tf.zeros((k,)) + alpha\n",
    "        beta_ = tf.zeros(()) + beta\n",
    "        _, wt, ll = train(data_enc, alpha_, beta_, n, k, V, max_it=250, parallel_iterations=2)\n",
    "        phi = get_phi_matrix(wt, beta_, V)\n",
    "        \n",
    "        topics, weights = get_topics(phi, word_dict, k=k)\n",
    "        perplexity = np.exp(-ll/N)\n",
    "        coherence = get_coherence(topics, data_proc, word_dict, 'c_v')\n",
    "        \n",
    "        str_topics = repr([list(topic) for topic in topics])\n",
    "        str_weights = repr([list(weight) for weight in weights])\n",
    "        results.loc[i] = n, k, alpha, beta, ll.numpy(), perplexity, coherence, str_topics, str_weights\n",
    "        \n",
    "        print(\"Perplexity:\", perplexity, \"Coherence:\", coherence)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    results.to_csv(\"results/resultset_n{}.csv\".format(n, k), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_4000 = pd.read_csv(\"results/resultset_n4000.csv\")\n",
    "df_results_4000.sort_values(by=\"coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results_4000.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_results_4000.K, df_results_4000.coherence)\n",
    "ax.set_xlabel(\"Topics\")\n",
    "ax.set_xticks(df_results_4000.K)\n",
    "ax.set_ylabel(\"Coherence\")\n",
    "\n",
    "ax.set_title(\"Topic Number vs. Coherence - 4000 Documents\")\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_4000-TopicVsCoherence.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results_4000.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_results_4000.K, df_results_4000.coherence)\n",
    "ax.set_xlabel(\"Topics\")\n",
    "ax.set_xticks(df_results_4000.K)\n",
    "ax.set_ylabel(\"Perplexity\")\n",
    "\n",
    "ax.set_title(\"Topic Number vs. Perplexity - 4000 Documents\")\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_4000-TopicVsPerplexity.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "df_results_4000.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[0].plot(df_results_4000.K, df_results_4000.alpha)\n",
    "axes[0].set_xlabel(\"Topics\")\n",
    "axes[0].set_ylabel(\"$\\\\alpha$\")\n",
    "\n",
    "df_results_4000.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[1].plot(df_results_4000.K, df_results_4000.eta)\n",
    "axes[1].set_xlabel(\"Topics\")\n",
    "axes[1].set_ylabel(\"$\\\\beta$\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(df_results_4000.K)\n",
    "    \n",
    "fig.suptitle(\"Optimized priors vs. Topics - 25 documents\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_25-TopicVsOptPriors.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_50 = pd.read_csv(\"results/resultset_topicVariation_50.csv\")\n",
    "df_results_50.sort_values(by=\"coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results_50.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_results_50.K, df_results_50.coherence)\n",
    "ax.set_xlabel(\"Topics\")\n",
    "ax.set_xticks(df_results_50.K)\n",
    "ax.set_ylabel(\"Coherence\")\n",
    "\n",
    "ax.set_title(\"Topic Number vs. Coherence - 50 Documents\")\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_50-TopicVsCoherence.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "df_results_50.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[0].plot(df_results_50.K, df_results_50.alpha)\n",
    "axes[0].set_xlabel(\"Topics\")\n",
    "axes[0].set_ylabel(\"$\\\\alpha$\")\n",
    "\n",
    "df_results_50.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[1].plot(df_results_50.K, df_results_50.eta)\n",
    "axes[1].set_xlabel(\"Topics\")\n",
    "axes[1].set_ylabel(\"$\\eta$\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(df_results_50.K)\n",
    "    \n",
    "fig.suptitle(\"Optimized priors vs. Topics - 50 documents\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_50-TopicVsOptPriors.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_100 = pd.read_csv(\"results/resultset_topicVariation_100.csv\")\n",
    "df_results_100.sort_values(by=\"coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results_100.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_results_100.K, df_results_100.coherence)\n",
    "ax.set_xlabel(\"Topics\")\n",
    "ax.set_xticks(df_results_100.K)\n",
    "ax.set_ylabel(\"Coherence\")\n",
    "\n",
    "ax.set_title(\"Topic Number vs. Coherence - 100 Documents\")\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_100-TopicVsCoherence.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(8,4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "df_results_100.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[0].plot(df_results_100.K, df_results_100.alpha)\n",
    "axes[0].set_xlabel(\"Topics\")\n",
    "axes[0].set_ylabel(\"$\\\\alpha$\")\n",
    "\n",
    "df_results_100.sort_values(by=\"K\", ascending=True, inplace=True)\n",
    "axes[1].plot(df_results_100.K, df_results_100.eta)\n",
    "axes[1].set_xlabel(\"Topics\")\n",
    "axes[1].set_ylabel(\"$\\eta$\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(df_results_100.K)\n",
    "    \n",
    "fig.suptitle(\"Optimized priors vs. Topics - 100 documents\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(\"results/resultset_topicVariation_100-TopicVsOptPriors.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAX_IT = 10\n",
    "# EPS = 0.001\n",
    "# tf.executing_eagerly()\n",
    "# optim = tf.keras.optimizers.Adam(1e-3)\n",
    "# # B = ed.Dirichlet(concentration=tf.fill([K, V], 0.1), name=\"topics\")\n",
    "# # Z = ed.DirichletMultinomial(tf.convert_to_tensor(Ns), concentration=tf.fill([D, K], 0.1))\n",
    "# alpha = np.copy(alpha_n).astype(np.float32)\n",
    "# eta = np.copy(eta_n).astype(np.float32)\n",
    "\n",
    "# beta = np.copy(beta_n).astype(np.float32)\n",
    "# phi = [np.full((n, K), 1/K).astype(np.float32) for n in Ns]\n",
    "# gamma = np.copy(gamma_n).astype(np.float32)\n",
    "# lmbd = np.copy(lmbd_n).astype(np.float32)\n",
    "\n",
    "# bb = None\n",
    "# gg = None\n",
    "# ww = None\n",
    "\n",
    "# class Positive(tf.keras.constraints.Constraint):\n",
    "#     def __call__(self, w):\n",
    "#         return w * tf.cast(tf.math.greater(w, 0.), w.dtype)\n",
    "\n",
    "# bb = []\n",
    "# for it in trange(MAX_IT):\n",
    "#     bb2 = []\n",
    "#     gg2 = []\n",
    "#     ww2 = []\n",
    "#     print('before', gamma)\n",
    "#     for d in range(D):\n",
    "#         for n in range(Ns[d]):\n",
    "#             for i in range(K):\n",
    "#                 phi[d][n, i] = beta[i, data_enc.iloc[d][n]] * np.exp(digamma(gamma[d, i]) - digamma(np.sum(gamma[d])))\n",
    "#         phi[d] /= np.sum(phi[d], axis=-1, keepdims=True) + 1e-5\n",
    "        \n",
    "#         for i in range(K):\n",
    "#             gamma[d, i] = alpha[i] + np.sum(phi[d][:, i])\n",
    "#     print('after', gamma)\n",
    "\n",
    "#     lmbd = np.full((K, V), eta)\n",
    "#     for i in range(K):\n",
    "#         for j in range(V):\n",
    "#             for d in range(D):\n",
    "#                 mask = (data_enc.iloc[d] == j)\n",
    "#                 lmbd[i, j] += np.sum(phi[d][:, i]*mask)\n",
    "    \n",
    "     \n",
    "# #     if bb is None:\n",
    "# #         bb = bb2\n",
    "# #         gg = gg2\n",
    "# #         ww = ww2\n",
    "# #         break\n",
    "                \n",
    "#     alpha_t = tf.Variable(alpha, trainable=True, constraint=Positive())\n",
    "#     gamma_t = tf.convert_to_tensor(gamma, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "#     def f_x():\n",
    "#         g_term = tf.math.reduce_sum(tf.expand_dims((alpha_t - 1), 0)*(tf.math.digamma(gamma_t) - \n",
    "#                                                    tf.math.digamma(tf.math.reduce_sum(gamma_t, axis=1, keepdims=True))), axis=1)\n",
    "#         loss = -tf.math.reduce_sum(tf.math.lgamma(tf.math.reduce_sum(alpha_t)) - tf.math.reduce_sum(tf.math.lgamma(alpha_t)) + g_term)\n",
    "#         return loss\n",
    "    \n",
    "#     for itt in range(10):\n",
    "#         for i in range(K):\n",
    "#             for itt1 in range(50):\n",
    "#                 #with tf.GradientTape() as tape:\n",
    "#                 optim.minimize(f_x, [alpha_t])\n",
    "# #                 grads = tape.gradient(loss, opt_a)\n",
    "# #                 optim.apply_gradients([(grads, opt_a)])\n",
    "#                 alpha[i] = alpha_t.numpy()[i]\n",
    "#                 np.nan_to_num(alpha, copy=False, nan=1e-5)\n",
    "#                 alpha_t.assign(alpha)\n",
    "#         print(alpha_t)\n",
    "#     beta = (lmbd - eta) / (np.sum(lmbd - eta, axis=-1, keepdims=True) + 1e-5)\n",
    "#     break\n",
    "    \n",
    "#     eta_t = tf.Variable(eta, trainable=True, constraint=Positive())\n",
    "    \n",
    "#     @tf.function\n",
    "#     def f_eta():\n",
    "#         loss = K*((eta_t-1)*(tf.math.digamma(eta_t) - tf.math.digamma(eta_t*V)) + tf.math.lgamma(eta_t*V) - V*tf.math.lgamma(eta))\n",
    "#         return loss\n",
    "    \n",
    "#     for itt1 in range(50):\n",
    "#         optim.minimize(f_eta, [eta_t])\n",
    "#     eta = eta_t.numpy()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
