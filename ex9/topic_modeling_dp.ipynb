{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iron/Repositories/classes/ml_2_classes/ML2_Exercises/pgm/ex9/.env/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.notebook import trange\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import norm as normal\n",
    "from pickle import dump, load\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing import strip_tags, strip_numeric, strip_multiple_whitespaces, stem_text, strip_punctuation, remove_stopwords\n",
    "from gensim.parsing import preprocess_string\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/iron/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/iron/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/iron/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
       "1    683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
       "2    394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
       "3    Bayesian Query Construction for Neural\\nNetwor...\n",
       "4    Neural Network Ensembles, Cross\\nValidation, a...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "\n",
    "seed = 42\n",
    "data_train = pd.read_csv(\"data/papers.csv\")[\"paper_text\"]\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...\n",
       "1       683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...\n",
       "2       394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...\n",
       "3       Bayesian Query Construction for Neural\\nNetwor...\n",
       "4       Neural Network Ensembles, Cross\\nValidation, a...\n",
       "                              ...                        \n",
       "7236    Single Transistor Learning Synapses\\n\\nPaul Ha...\n",
       "7237    Bias, Variance and the Combination of\\nLeast S...\n",
       "7238    A Real Time Clustering CMOS\\nNeural Engine\\nT....\n",
       "7239    Learning direction in global motion: two\\nclas...\n",
       "7240    Correlation and Interpolation Networks for\\nRe...\n",
       "Name: paper_text, Length: 7241, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Learning Monotonic Transformations for\\nClassi...\n",
       "1    Selective Attention for Handwritten\\nDigit Rec...\n",
       "2    Predicting User Activity Level In Point Proces...\n",
       "3    Learning Deep Parsimonious Representations\\n\\n...\n",
       "4    Sketch-Based Linear Value Function Approximati...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.Series(data_train).sample(n).copy()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stem_filters = [strip_tags,\n",
    "                        strip_numeric,\n",
    "                        strip_punctuation, \n",
    "                        lambda x: x.lower(),\n",
    "                        lambda s: re.sub(r'\\b\\w{1,2}\\b', ' ', s),\n",
    "                        strip_multiple_whitespaces,\n",
    "                        remove_stopwords\n",
    "                     ]\n",
    "\n",
    "def text_processing(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(i) for i in preprocess_string(document, clean_stem_filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_func(data):\n",
    "    with tqdm_joblib(tqdm(desc=\"Preprocessing\", total=len(data))) as progress_bar:\n",
    "        data_proc = Parallel(n_jobs=1)(delayed(text_processing)(text) for text in data)\n",
    "        data_proc = pd.Series(data_proc, index=data.index, name='data')\n",
    "    return data_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1afa54cbc1488a87f2dcde29bf32dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    [learning, monotonic, transformation, classifi...\n",
       "1    [selective, attention, handwritten, digit, rec...\n",
       "2    [predicting, user, activity, level, point, pro...\n",
       "3    [learning, deep, parsimonious, representation,...\n",
       "4    [sketch, based, linear, value, function, appro...\n",
       "Name: data, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc = proc_func(data)\n",
    "data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode2(text, word_dict):\n",
    "    return np.asarray(word_dict.doc2idx(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5bc2697d1248e59a62daa12265b89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_dict = Dictionary(data_proc)\n",
    "data_enc = data_proc.progress_apply(lambda x: encode2(x, word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(data_enc)\n",
    "Ns = data_enc.apply(lambda x: len(x)).to_numpy().astype(int)\n",
    "N = Ns.sum()\n",
    "V = len(word_dict)\n",
    "Gs = [None] * D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [1604  592 2862 2611 2446 2589 2907 2131 1608 1799 2259 2395 1047 1321\n",
      " 2196 1310 2696 1282  801 2132 3419 2015 1826 3547 2407 1780 2038 1516\n",
      " 1193  777 2846 1828 1572 2541 2847 2528 2827 1773 1419 2530  700 2764\n",
      "  660 1116 2436  808  559 1652 2088 1526 1293 1869 1726 1342 2565 2642\n",
      " 2086 2649 1453 2138 2054 1521 2602 2845 3020 2746  994 1920 1290 2400\n",
      " 2016 2622 1118 1811 2468 2720 2158 2841 3351 1255 1848 2768 2135 2384\n",
      " 1469 1850  936 1339 2267 1547 1985 3425 1183 2510 2061 2477 1228 2846\n",
      " 2941 2844] 201684 14001 100\n"
     ]
    }
   ],
   "source": [
    "print(D, Ns, N, V, len(Gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP0:\n",
    "    def __init__(self, a, H):\n",
    "        self.a = a\n",
    "        self.H = H\n",
    "        self.K = 0 # Dishes\n",
    "        self.phi_k = [] # Dish ID\n",
    "        self.t_k = [] # Number of tables serving dish K\n",
    "\n",
    "    def sample(self):\n",
    "        denom = 1./(len(self.t_k)+self.a)\n",
    "        alphas = np.asarray(self.t_k + [self.a]) * denom\n",
    "        probs = np.random.dirichlet(alphas)\n",
    "        k = np.random.choice(range(self.K+1), p=probs)\n",
    "        if k+1 > self.K:\n",
    "            self.K += 1\n",
    "            self.phi_k.append(H.rvs())\n",
    "\n",
    "        return self.phi_k[k], k\n",
    "    \n",
    "    def increase_dish_count(self, dish):\n",
    "        if len(self.t_k) <= dish:\n",
    "            self.t_k += [0] * ((dish+1) - len(self.t_k))\n",
    "        self.t_k[dish] += 1\n",
    "\n",
    "    def decrease_dish_count(self, dish):\n",
    "        if len(self.t_k) > dish:\n",
    "            self.t_k[dish] -= 1\n",
    "\n",
    "class DP1:\n",
    "    def __init__(self, a, DP, words):\n",
    "        self.a = a\n",
    "        self.G = DP\n",
    "        self.t = 0\n",
    "        self.psi_t = []\n",
    "        self.m = []\n",
    "        self.n_tk = []\n",
    "        self.k_t = []\n",
    "        self.theta_i = [None] * len(words)\n",
    "        self.words = words\n",
    "        \n",
    "        self._init_sample()\n",
    "        \n",
    "    def _init_sample(self):\n",
    "        for i in range(len(self.words)):\n",
    "            denom = 1./(i+self.a)\n",
    "            if len(self.n_tk) == 0:\n",
    "                alphas = [self.a * denom]\n",
    "            else:\n",
    "                alphas = np.asarray([np.sum(t) for t in self.n_tk] + [self.a]) * denom\n",
    "            probs = np.random.dirichlet(alphas)\n",
    "            t = np.random.choice(range(self.t+1), p=probs)\n",
    "            if t+1 > self.t:\n",
    "                self.t += 1\n",
    "                phi_k, k = self.G.sample()\n",
    "                self.psi_t.append(phi_k)\n",
    "                self.k_t.append(k)\n",
    "                if len(self.m) <= k:\n",
    "                    self.m.append(1)\n",
    "                else:\n",
    "                    self.m[k] += 1\n",
    "                if len(self.n_tk) <= self.t:\n",
    "                    self.n_tk.append([])\n",
    "            else:\n",
    "                self.psi_t.append(phi_k)\n",
    "                self.k_t.append(k)\n",
    "\n",
    "            if len(self.n_tk[t]) <= k:\n",
    "                    self.n_tk[t] += [0] * ((k+1)-len(self.n_tk[t]))\n",
    "                    \n",
    "            self.n_tk[t][k] += 1\n",
    "            self.G.increase_dish_count(k)\n",
    "            self.theta_i[i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, <scipy.stats._distn_infrastructure.rv_frozen at 0x7ff7c7beeee0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = normal(0, 1/2)\n",
    "a = 5\n",
    "a, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DP0(a, H)\n",
    "gjs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_enc:\n",
    "    gjs.append(DP1(5, dp, list(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjs[0].t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
