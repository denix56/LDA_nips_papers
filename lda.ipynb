{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.notebook import trange\n",
    "import contextlib\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import edward2 as ed\n",
    "import tensorflow as tf\n",
    "from scipy.special import digamma\n",
    "from pickle import dump, load\n",
    "from scipy.sparse import csr_matrix\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# tensorflow does not work with new numpy versions\n",
    "assert np.__version__  < '1.20'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/senkin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc = pd.read_pickle('data_proc.pkl')\n",
    "# data_enc = pd.read_pickle('data.pkl')\n",
    "# with open('word_to_idx.pkl', 'rb') as f:\n",
    "#     words_to_idx = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "data = pd.read_csv('nips-papers/papers.csv')\n",
    "data = data[['paper_text']]\n",
    "data = data.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\d+)', '', text)\n",
    "    text = re.sub(r'(\\n)|(\\t)', ' ', text)\n",
    "    text = text.translate({ ord(c): None for c in string.punctuation })\n",
    "    text = text.strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    text = [i for i in tokens if not i in stop_words and len(i) > 1]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5067e9c8b1da46e4a3b1a4624a8ed522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_proc = data['paper_text'].progress_apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509     [independent, component, analysis, identificat...\n",
       "2576    [nearmaximum, entropy, model, binary, neural, ...\n",
       "6362    [nearestneighbor, sample, compression, efficie...\n",
       "6173    [efficient, highorder, interactionaware, featu...\n",
       "6552    [multioutput, polynomial, network, factorizati...\n",
       "Name: paper_text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917f8eaf05fa4f4d9a0dc8914b350ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_to_idx = {}\n",
    "idx_to_word = {}\n",
    "def encode(text, words_to_idx, idx_to_word):\n",
    "    text_enc = np.empty(len(text), dtype=int)\n",
    "    for i, word in enumerate(text):\n",
    "        if word not in words_to_idx:\n",
    "            idx = words_to_idx[word] = len(words_to_idx)\n",
    "            idx_to_word[idx] = word\n",
    "        else:\n",
    "            idx = words_to_idx[word]\n",
    "        text_enc[i] = idx\n",
    "    return text_enc\n",
    "data_enc = data_proc.progress_apply(encode, args=(words_to_idx, idx_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc.to_pickle('data_proc.pkl')\n",
    "# data_enc.to_pickle('data.pkl')\n",
    "\n",
    "# with open('word_to_idx.pkl', 'wb') as f:\n",
    "#     dump(words_to_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "D = len(data_enc)\n",
    "Ns = data_enc.apply(lambda x: len(x)).to_numpy().astype(np.int)\n",
    "N = Ns.sum()\n",
    "V = len(words_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse(data, alpha, D, K, V):\n",
    "    Ns = np.empty(D, dtype=np.int)\n",
    "     \n",
    "    for i, doc in enumerate(data):\n",
    "        Ns[i] = len(doc)\n",
    "    \n",
    "    N = Ns.sum()\n",
    "    rows = np.empty(N, dtype=np.int64)\n",
    "    cols = np.empty(N, dtype=np.int64)\n",
    "    v_cols = np.empty(N, dtype=np.int64)\n",
    "    \n",
    "    last_idx = 0\n",
    "    \n",
    "    for i, doc in tqdm(enumerate(data), total=D):\n",
    "        n = len(doc)\n",
    "        rows[last_idx:last_idx+n] = i\n",
    "        cols[last_idx:last_idx+n] = np.arange(n, dtype=np.int64)\n",
    "        v_cols[last_idx:last_idx+n] = doc\n",
    "        last_idx += n\n",
    "        \n",
    "    N_max = Ns.max()\n",
    "    phi_rows = np.repeat(rows, K)\n",
    "    phi_cols = np.repeat(cols, K)\n",
    "    phi3 = np.tile(np.arange(K), len(rows))  \n",
    "    phi_indices = np.vstack((phi_rows, phi_cols, phi3)).T\n",
    "    phi = tf.sparse.SparseTensor(phi_indices, values=tf.fill((len(phi_indices),), 1/K), dense_shape=(D, N_max, K))\n",
    "    phi = tf.sparse.reorder(phi)\n",
    "    gamma_v_cols = np.repeat(v_cols, K)\n",
    "    gamma = tf.expand_dims(alpha, 0) + (Ns / K).reshape(-1, 1)\n",
    "\n",
    "    beta_indices = np.vstack((phi3, gamma_v_cols)).T\n",
    "    gamma_indices = np.vstack((phi_rows, phi3)).T\n",
    "    w_indices = np.vstack((phi_rows, phi_cols, phi3, gamma_v_cols)).T\n",
    "    tmp = tf.sparse.SparseTensor(w_indices, phi.values, dense_shape=(D, N_max, K, V))\n",
    "    tmp = tf.sparse.reorder(tmp)\n",
    "    w_indices = tmp.indices\n",
    "    lmbd = tf.fill((K, V), 1/V)\n",
    "    return phi, gamma, lmbd, tf.constant(beta_indices, dtype=tf.int64), tf.constant(gamma_indices, dtype=tf.int64), tf.constant(w_indices, dtype=tf.int64), N_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_n = np.random.rand(K).astype(np.float32)\n",
    "# eta_n = np.random.rand(1).astype(np.float32)\n",
    "# beta_n = np.random.dirichlet(np.random.rand(V).astype(np.float32), size=K).astype(np.float32)\n",
    "# phi_n = [np.full((n, K), 1/K).astype(np.float32) for n in Ns]\n",
    "# gamma_n = alpha_n.reshape(1, -1) + Ns.reshape(-1, 1) / K\n",
    "# gammma_n = gamma_n.astype(np.float32)\n",
    "# lmbd_n = np.full((K, V), eta_n)\n",
    "# lmbd_n = lmbd_n.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48bb7b8faf3460b85f13639bd6931fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing...\n",
      "1 : E step\tM step\t"
     ]
    }
   ],
   "source": [
    "# It tries to estimate also smoothed params, but eta tends to go negative, breaking all the stuff.\n",
    "# Maybe loss is incorrect\n",
    "\n",
    "class Positive(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        return w * tf.cast(tf.math.greater(w, 0.), w.dtype)   \n",
    "    \n",
    "@tf.function\n",
    "def calc_elbo(alpha, beta, eta, phi, gamma, lmbd, w_indices, D, K, N_max, V):\n",
    "    digamma = tf.math.digamma(gamma) - tf.math.digamma(tf.math.reduce_sum(gamma, axis=1, keepdims=True))\n",
    "    ta = tf.math.lgamma(tf.math.reduce_sum(alpha)) - \\\n",
    "                                          tf.math.reduce_sum(tf.math.lgamma(alpha)) + \\\n",
    "                                          tf.math.reduce_sum(tf.expand_dims(alpha - 1, 0)*digamma, axis=1)\n",
    "    zt = tf.sparse.reduce_sum(tf.expand_dims(digamma, 1)*phi, axis=(1, 2))\n",
    "    wzb = tf.math.reduce_sum(tf.expand_dims(tf.math.log1p(beta), axis=0) * tf.sparse.reduce_sum(tf.SparseTensor(w_indices, phi.values, dense_shape=(D, N_max, K, V)), axis=(1)), axis=(1, 2))\n",
    "    bl = tf.math.reduce_sum((eta-1)*tf.math.reduce_sum(tf.math.digamma(lmbd) - tf.math.digamma(tf.math.reduce_sum(lmbd, axis=1, keepdims=True)), axis=1) + \\\n",
    "         tf.math.lgamma(eta*V) - V*tf.math.lgamma(eta))\n",
    "    qt = -tf.math.lgamma(tf.math.reduce_sum(gamma, axis=1)) + tf.math.reduce_sum(tf.math.lgamma(gamma), axis=1) - tf.math.reduce_sum((gamma-1)*digamma, axis=1)\n",
    "    phi_log_phi = tf.sparse.SparseTensor(phi.indices, phi.values*tf.math.log1p(phi.values), dense_shape=[D, N_max, K])\n",
    "    qz = tf.sparse.reduce_sum(phi_log_phi, axis=(1, 2))\n",
    "    elbo = tf.math.reduce_sum(ta + zt + qt + qz)\n",
    "    return elbo\n",
    "\n",
    "@tf.function\n",
    "def e_step_it(alpha, beta_mod, eta, phi, gamma, lmbd, gamma_indices, w_indices, D, K, N_max, V):\n",
    "    # phi\n",
    "    dg = tf.math.exp(tf.math.digamma(gamma) - tf.math.digamma(tf.math.reduce_sum(gamma, axis=1, keepdims=True)))\n",
    "    phi = tf.sparse.SparseTensor(phi.indices, beta_mod*tf.gather_nd(dg, gamma_indices), dense_shape=[D, N_max, K])\n",
    "    phi /= tf.sparse.reduce_sum(phi, axis=2, keepdims=True) + 1e-5\n",
    "    # gamma\n",
    "    gamma = tf.expand_dims(alpha, 0) + tf.sparse.reduce_sum(phi, axis=1)\n",
    "    # lambda\n",
    "    lmbd = eta + tf.sparse.reduce_sum(tf.SparseTensor(w_indices, phi.values, dense_shape=(D, N_max, K, V)), axis=(0, 1))\n",
    "    \n",
    "    gamma.set_shape((D, K))\n",
    "    lmbd.set_shape((K, V))\n",
    "    return phi, gamma, lmbd\n",
    "\n",
    "@tf.function\n",
    "def e_step(alpha, beta, eta, phi, gamma, lmbd, beta_indices, gamma_indices, w_indices, D, K, N_max, V, max_it=1000, rtol=1e-03, atol=1e-03):      \n",
    "    beta_mod = tf.gather_nd(beta, beta_indices)\n",
    "\n",
    "    np_isclose = lambda elbo_old, elbo: np.allclose(elbo_old, elbo, rtol=rtol, atol=atol)\n",
    "    tf_cond = lambda i, elbo_old, elbo, args: tf.logical_and(i < max_it, tf.logical_or(i == 0, tf.logical_not(tf.numpy_function(np_isclose, [elbo_old, elbo], tf.bool))))\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_body(i, elbo_old, elbo, args):\n",
    "        phi, gamma, lmbd = args\n",
    "        i = i + 1\n",
    "        elbo_old = tf.identity(elbo)\n",
    "        phi, gamma, lmbd = e_step_it(alpha, beta_mod, eta, phi, gamma, lmbd, gamma_indices, w_indices, D, K, N_max, V)\n",
    "        elbo = calc_elbo(alpha, beta, eta, phi, gamma, lmbd, w_indices, D, K, N_max, V)\n",
    "        args = (phi, gamma, lmbd)\n",
    "        return (i, elbo_old, elbo, args)\n",
    "    \n",
    "    i = tf.constant(0, name='e_loop_counter')\n",
    "    elbo = tf.constant(np.inf, dtype=tf.float32)\n",
    "    elbo_old = tf.identity(elbo)\n",
    "    args = (phi, gamma, lmbd)\n",
    "    \n",
    "    i, _, elbo, args = tf.while_loop(tf_cond, tf_body, [i, elbo_old, elbo, args], parallel_iterations=1)\n",
    "    phi, gamma, lmbd = args\n",
    "    return phi, gamma, lmbd\n",
    "\n",
    "@tf.function\n",
    "def m_step(alpha, alpha_2, beta, eta, phi, gamma, lmbd, w_indices, D, K, N_max, V, opt, max_it=1000, rtol=1e-03, atol=1e-03):\n",
    "    # beta\n",
    "    beta = tf.sparse.reduce_sum(tf.SparseTensor(w_indices, phi.values, dense_shape=(D, N_max, K, V)), axis=(0, 1))\n",
    "    beta /= tf.math.reduce_sum(beta, axis=1, keepdims=True) + 1e-5\n",
    "    # alpha\n",
    "    digamma = tf.math.reduce_sum(tf.math.digamma(gamma) - tf.math.digamma(tf.math.reduce_sum(gamma, axis=1, keepdims=True)), axis=0)\n",
    "    \n",
    "    @tf.function\n",
    "    def alpha_elbo(alpha, digamma):\n",
    "        val = tf.math.reduce_sum(tf.math.lgamma(tf.math.reduce_sum(alpha)) - \\\n",
    "                                          tf.math.reduce_sum(tf.math.lgamma(alpha)) + \\\n",
    "                                          tf.math.reduce_sum(tf.expand_dims(alpha - 1, 0)*digamma, axis=1))\n",
    "        return val\n",
    "    \n",
    "    alpha_it_cond = lambda i, digamma: i < K\n",
    "    \n",
    "    np_isclose = lambda elbo_old, elbo: np.allclose(elbo_old, elbo, rtol=rtol, atol=atol)\n",
    "    alpha_cond = lambda i, elbo_old, elbo, digamma: tf.logical_and(i < max_it, tf.logical_or(i == 0, tf.logical_not(tf.numpy_function(np_isclose, [elbo_old, elbo], tf.bool))))\n",
    "    \n",
    "    @tf.function\n",
    "    def alpha_it_body(i, digamma):        \n",
    "        def loss_func():\n",
    "            ta = tf.TensorArray(alpha.dtype, size=K, dynamic_size=False)\n",
    "            alpha_copy = ta.unstack(alpha_2)\n",
    "            alpha_copy = alpha_copy.write(i, tf.math.maximum(alpha[i], 1e-5))\n",
    "            alpha_copy = alpha_copy.stack()\n",
    "            alpha.assign(alpha_copy)\n",
    "            alpha_2.assign(alpha_copy)\n",
    "            a = tf.math.reduce_sum((alpha - 1)*digamma)\n",
    "            res = -(D*(tf.math.lgamma(tf.math.reduce_sum(alpha)) - tf.math.reduce_sum(tf.math.lgamma(alpha))) + a)\n",
    "            return res\n",
    "        losses = tfp.math.minimize(loss_func, max_it, opt, convergence_criterion=tfp.optimizer.convergence_criteria.LossNotDecreasing(rtol=rtol, atol=atol), \n",
    "                                   trainable_variables=[alpha])\n",
    "        \n",
    "        ta = tf.TensorArray(alpha.dtype, size=K, dynamic_size=False)\n",
    "        alpha_copy = ta.unstack(alpha_2)\n",
    "        alpha_copy = alpha_copy.write(i, tf.math.maximum(alpha[i], 1e-5))\n",
    "        alpha_copy = alpha_copy.stack()\n",
    "        alpha.assign(alpha_copy)\n",
    "        \n",
    "        return i + 1, digamma\n",
    "    \n",
    "    @tf.function\n",
    "    def alpha_body(i, elbo_old, elbo, digamma):\n",
    "        i = i + 1\n",
    "        j = tf.constant(0)\n",
    "        elbo_old = tf.identity(elbo)\n",
    "        j, _ = tf.while_loop(alpha_it_cond, alpha_it_body, [j, digamma], parallel_iterations=1)\n",
    "#         tf.print(alpha, output_stream=sys.stdout)\n",
    "        elbo = alpha_elbo(alpha, digamma)\n",
    "        return (i, elbo_old, elbo, digamma)\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    elbo = tf.constant(np.inf, dtype=tf.float32)\n",
    "    elbo_old = tf.identity(elbo)\n",
    "    i, _, elbo, _ = tf.while_loop(alpha_cond, alpha_body, [i, elbo_old, elbo, digamma], parallel_iterations=1)\n",
    "    \n",
    "    di_lmbd = tf.math.reduce_sum(tf.math.digamma(lmbd) - tf.math.digamma(tf.math.reduce_sum(lmbd, axis=1, keepdims=True))) \n",
    "    # eta\n",
    "    def loss_func():\n",
    "        loss_v = -((eta-1)*di_lmbd + K*(tf.math.lgamma(eta*V) - V*tf.math.lgamma(eta)))\n",
    "        return loss_v\n",
    "    \n",
    "    losses = tfp.math.minimize(loss_func, max_it, opt, convergence_criterion=tfp.optimizer.convergence_criteria.LossNotDecreasing(rtol=rtol, atol=atol), trainable_variables=[eta])\n",
    "    eta.assign(tf.math.maximum(eta, 0.1))\n",
    "    return beta\n",
    "\n",
    "def train(data, D, K, V, max_it=1000, seed=42, rtol=1e-3, atol=1e-3):\n",
    "    start = time.time()\n",
    "    alpha = tf.random.uniform((K,))\n",
    "    eta = tf.random.uniform((1,))\n",
    "    beta = tf.convert_to_tensor(np.random.dirichlet(np.random.rand(V), size=K), dtype=tf.float32)\n",
    "    opt = tf.optimizers.Adam(1e-3)\n",
    "    \n",
    "    phi, gamma, lmbd, beta_indices, gamma_indices, w_indices, N_max = create_sparse(data, alpha, D, K, V)\n",
    "    N_max = N_max.item()\n",
    "    \n",
    "    alpha = tf.Variable(alpha, constraint=Positive())\n",
    "    alpha_2 = tf.Variable(tf.identity(alpha), trainable=False)\n",
    "    eta = tf.Variable(eta, constraint=Positive())\n",
    "\n",
    "    np_isclose = lambda elbo_old, elbo: np.allclose(elbo_old, elbo, rtol=1e-3, atol=1e-3)\n",
    "    train_cond = lambda i, elbo_old, elbo, args: tf.logical_and(i < max_it, tf.logical_or(i == 0, tf.logical_not(tf.numpy_function(np_isclose, [elbo_old, elbo], tf.bool))))\n",
    "    \n",
    "    @tf.function\n",
    "    def train_body(i, elbo_old, elbo, args):\n",
    "        i = i + 1\n",
    "        beta, phi, gamma, lmbd = args\n",
    "        elbo_old = elbo\n",
    "        tf.print(i, ': E step', output_stream=sys.stdout, end='\\t')\n",
    "        phi, gamma, lmbd = e_step(alpha, beta, eta, phi, gamma, lmbd, beta_indices, gamma_indices, w_indices, D, K, N_max, V, atol=atol, rtol=rtol)\n",
    "        tf.print('M step', output_stream=sys.stdout, end='\\t')\n",
    "        beta = m_step(alpha, alpha_2, beta, eta, phi, gamma, lmbd, w_indices, D, K, N_max, V, opt, atol=atol, rtol=rtol)\n",
    "        elbo = calc_elbo(alpha, beta, eta, phi, gamma, lmbd, w_indices, D, K, N_max, V)\n",
    "        tf.print('ELBO =', elbo, output_stream=sys.stdout)\n",
    "        args = (beta, phi, gamma, lmbd)\n",
    "        return i, elbo_old, elbo, args\n",
    "    \n",
    "    i = tf.constant(0)\n",
    "    args = (beta, phi, gamma, lmbd)\n",
    "    elbo = tf.constant(np.inf, dtype=tf.float32)\n",
    "    elbo_old = tf.identity(elbo)\n",
    "    print('Preparing...')\n",
    "    i, elbo_old, elbo, args = tf.while_loop(train_cond, train_body, [i, elbo_old, elbo, args], parallel_iterations=1)\n",
    "    beta, phi, gamma, lmbd = args\n",
    "    tf.print('Converged in', i,  'iterations', output_stream=sys.stdout)\n",
    "    end = time.time()\n",
    "    print('Time:', end-start, 's')\n",
    "    return alpha, beta, eta, phi, gamma, lmbd, elbo\n",
    "\n",
    "alpha, beta, eta, phi, gamma, lmbd, elbo = train(data_enc, D, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.4967644], dtype=float32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get topic top words\n",
    "for h in range(K):\n",
    "    print(([idx_to_word[i] for i in tf.math.top_k(beta[h], k=10)[1].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n",
      "hmm\n",
      "extreme\n",
      "activation\n",
      "full\n",
      "dead\n",
      "equilibrium\n",
      "case\n",
      "weight\n",
      "qn\n",
      "specific\n",
      "exist\n",
      "symmetric\n",
      "maxproduct\n",
      "plain\n",
      "sampled\n",
      "like\n",
      "fii\n",
      "gatsby\n",
      "assigns\n",
      "problem\n",
      "leaf\n",
      "weightsharing\n",
      "distribution\n",
      "respectively\n",
      "hinton\n",
      "algorithm\n",
      "similar\n",
      "unobserved\n",
      "using\n",
      "sequence\n",
      "standard\n",
      "bk\n",
      "function\n",
      "network\n",
      "moemicrosoft\n",
      "backpropagation\n",
      "regression\n",
      "function\n",
      "operation\n",
      "rissanen\n",
      "one\n",
      "set\n",
      "science\n",
      "end\n",
      "music\n",
      "university\n",
      "mean\n",
      "sequence\n",
      "example\n",
      "node\n",
      "stationary\n",
      "rtdp\n",
      "corresponds\n",
      "weight\n",
      "process\n",
      "ensure\n",
      "yield\n",
      "representation\n",
      "update\n",
      "\u001b\u0018\n",
      "wi\n",
      "hidden\n",
      "linear\n",
      "section\n",
      "distribution\n",
      "ie\n",
      "fpf\n",
      "wn\n",
      "take\n",
      "paper\n",
      "sign\n",
      "marcus\n",
      "choice\n",
      "sli\n",
      "vague\n",
      "chosen\n",
      "covu\n",
      "across\n",
      "method\n",
      "pair\n",
      "sn\n",
      "taking\n",
      "one\n",
      "perturbing\n",
      "mixture\n",
      "examined\n",
      "study\n",
      "like\n",
      "structure\n",
      "structured\n",
      "rest\n",
      "stochastic\n",
      "intelligence\n",
      "ancestor\n",
      "total\n",
      "gibbsmh\n",
      "consider\n",
      "phase\n",
      "split\n",
      "regularization\n",
      "depend\n",
      "mechanic\n",
      "logmse\n",
      "bound\n",
      "larger\n",
      "ann\n",
      "multistate\n",
      "tensor\n",
      "us\n",
      "reinforcement\n",
      "weight\n",
      "operator\n",
      "vector\n",
      "data\n",
      "prerequisite\n",
      "interference\n",
      "state\n",
      "transitioned\n",
      "suboptimal\n",
      "qjz\n",
      "organisation\n",
      "remainder\n",
      "rate\n",
      "neural\n",
      "appear\n",
      "finite\n",
      "parameter\n",
      "fokkerplanck\n",
      "number\n",
      "relaxation\n",
      "sequence\n",
      "variable\n",
      "stuck\n",
      "dynamical\n",
      "ie\n",
      "problem\n",
      "applying\n",
      "iterative\n",
      "little\n",
      "saddle\n",
      "several\n",
      "negative\n",
      "volume\n",
      "map\n",
      "markov\n",
      "size\n",
      "domain\n",
      "least\n",
      "msequences\n",
      "importance\n",
      "decision\n",
      "tree\n",
      "every\n",
      "ip\n",
      "activation\n",
      "weight\n",
      "neighbourhood\n",
      "find\n",
      "mode\n",
      "image\n",
      "needle\n",
      "xji\n",
      "output\n",
      "manuscript\n",
      "consists\n",
      "validation\n",
      "random\n",
      "tsp\n",
      "loglikelihoods\n",
      "ci\n",
      "figure\n",
      "would\n",
      "thus\n",
      "performance\n",
      "optimising\n",
      "scheme\n",
      "wakoshi\n",
      "xk\n",
      "relu\n",
      "following\n",
      "figure\n",
      "show\n",
      "year\n",
      "state\n",
      "present\n",
      "node\n",
      "iii\n",
      "exponential\n",
      "correlation\n",
      "maximally\n",
      "network\n",
      "rather\n",
      "without\n",
      "experimental\n",
      "timestep\n",
      "dot\n",
      "considerable\n",
      "qnn\n",
      "whole\n",
      "space\n",
      "maintain\n",
      "two\n",
      "tli\n",
      "reference\n",
      "search\n",
      "industrial\n",
      "snake\n",
      "model\n",
      "activation\n",
      "allows\n",
      "identification\n",
      "ji\n",
      "hidden\n",
      "method\n",
      "weight\n",
      "letter\n",
      "hidden\n",
      "domain\n",
      "proposal\n",
      "table\n",
      "need\n",
      "neural\n",
      "sampling\n",
      "pda\n",
      "database\n",
      "gibbs\n",
      "many\n",
      "call\n",
      "proposition\n",
      "pick\n",
      "system\n",
      "structure\n",
      "congress\n",
      "ming\n",
      "average\n",
      "positive\n",
      "model\n",
      "point\n",
      "learn\n",
      "dynamic\n",
      "feature\n",
      "deviation\n",
      "assignment\n",
      "network\n",
      "instance\n",
      "pw\n",
      "point\n",
      "agglomerative\n",
      "neuroscience\n",
      "hidden\n",
      "perturbed\n",
      "focus\n",
      "minimized\n",
      "regularizer\n",
      "fnonspur\n",
      "recurrent\n",
      "ith\n",
      "number\n",
      "catalan\n",
      "multiclass\n",
      "done\n",
      "update\n",
      "set\n",
      "mj\n",
      "region\n",
      "stam\n",
      "distributed\n",
      "problem\n",
      "jr\n",
      "product\n",
      "parse\n",
      "concentrated\n",
      "able\n",
      "science\n",
      "algorithm\n",
      "jd\n",
      "ergodic\n",
      "incremental\n",
      "metropolishastings\n",
      "time\n",
      "press\n",
      "japan\n",
      "harmony\n",
      "represents\n",
      "sample\n",
      "legendre\n",
      "topic\n",
      "digit\n",
      "shown\n",
      "describe\n",
      "metric\n",
      "represent\n",
      "section\n",
      "problem\n",
      "px\n",
      "document\n",
      "dp\n",
      "ratio\n",
      "gr\n",
      "evolution\n",
      "best\n",
      "label\n",
      "first\n",
      "measure\n",
      "lm\n",
      "nd\n",
      "count\n",
      "whenever\n",
      "process\n",
      "state\n",
      "neural\n",
      "review\n",
      "steven\n",
      "top\n",
      "finite\n",
      "model\n",
      "classifier\n",
      "column\n",
      "dp\n",
      "language\n",
      "resolved\n",
      "index\n",
      "typically\n",
      "model\n",
      "defined\n",
      "enhancement\n",
      "xk\n",
      "perfectly\n",
      "alignment\n",
      "exceeds\n",
      "evolve\n",
      "vector\n",
      "factor\n",
      "proceeding\n",
      "policy\n",
      "averaging\n",
      "qiearning\n",
      "cross\n",
      "hd\n",
      "completely\n",
      "iteration\n",
      "statistical\n",
      "classified\n",
      "algorithm\n",
      "merge\n",
      "previously\n",
      "useful\n",
      "integrated\n",
      "smooth\n",
      "duff\n",
      "clustering\n",
      "literature\n",
      "data\n",
      "left\n",
      "event\n",
      "matrix\n",
      "estimation\n",
      "\u000f\u0013\n",
      "unit\n",
      "considering\n",
      "practical\n",
      "section\n",
      "optimal\n",
      "approximation\n",
      "ti\n",
      "test\n",
      "set\n",
      "parse\n",
      "delivered\n",
      "model\n",
      "rumeihart\n",
      "guaranteed\n",
      "classifies\n",
      "recurrent\n",
      "output\n",
      "achieve\n",
      "multilabel\n",
      "reject\n",
      "\u0007\u0005\n",
      "toporstack\n",
      "smallsignal\n",
      "multidimensional\n",
      "smoothing\n",
      "estimate\n",
      "four\n",
      "except\n",
      "sky\n",
      "spline\n",
      "simple\n",
      "phonetic\n",
      "general\n",
      "probability\n",
      "hinton\n",
      "hidden\n",
      "daj\n",
      "graphical\n",
      "undesirable\n",
      "different\n",
      "data\n",
      "gd\n",
      "little\n",
      "deterministic\n",
      "transition\n",
      "ie\n",
      "local\n",
      "experiment\n",
      "procedure\n",
      "role\n",
      "language\n",
      "example\n",
      "property\n",
      "time\n",
      "estimate\n",
      "network\n",
      "defined\n",
      "exp\n",
      "qzk\n",
      "show\n",
      "machine\n",
      "diffusion\n",
      "still\n",
      "event\n",
      "optimal\n",
      "using\n",
      "identity\n",
      "intervalcensoring\n",
      "time\n",
      "transition\n",
      "compare\n",
      "sample\n",
      "variable\n",
      "point\n",
      "network\n",
      "neural\n",
      "concluded\n",
      "left\n",
      "artifact\n",
      "support\n",
      "function\n",
      "presented\n",
      "metric\n",
      "san\n",
      "generalize\n",
      "observe\n",
      "unexpected\n",
      "ti\n",
      "machine\n",
      "likelihood\n",
      "qiearning\n",
      "latent\n",
      "several\n",
      "learning\n",
      "may\n",
      "fact\n",
      "random\n",
      "iw\n",
      "error\n",
      "symbol\n",
      "dynamic\n",
      "proc\n",
      "ddw\n",
      "dependence\n",
      "given\n",
      "neuron\n",
      "since\n",
      "exemplified\n",
      "overall\n",
      "px\n",
      "matrix\n",
      "referred\n",
      "binary\n",
      "separate\n",
      "compressed\n",
      "combined\n",
      "line\n",
      "decay\n",
      "drift\n",
      "left\n",
      "leastsquares\n",
      "update\n",
      "learning\n",
      "approach\n",
      "jx\n",
      "path\n",
      "stochastic\n",
      "performance\n",
      "transition\n",
      "weight\n",
      "aim\n",
      "dirichlet\n",
      "determined\n",
      "network\n",
      "example\n",
      "network\n",
      "window\n",
      "le\n",
      "event\n",
      "input\n",
      "time\n",
      "consulting\n",
      "property\n",
      "variable\n",
      "element\n",
      "order\n",
      "feature\n",
      "application\n",
      "prior\n",
      "example\n",
      "figure\n",
      "assumed\n",
      "st\n",
      "observation\n",
      "example\n",
      "label\n",
      "gamma\n",
      "network\n",
      "perform\n",
      "variable\n",
      "report\n",
      "stable\n",
      "theory\n",
      "variable\n",
      "adaptive\n",
      "vol\n",
      "data\n",
      "robot\n",
      "subtracting\n",
      "used\n",
      "end\n",
      "focused\n",
      "overcome\n",
      "column\n",
      "branch\n",
      "effective\n",
      "represent\n",
      "model\n",
      "underlying\n",
      "statistic\n",
      "initialized\n",
      "ht\n",
      "character\n",
      "volume\n",
      "previous\n",
      "snp\n",
      "synthetically\n",
      "distribution\n",
      "system\n",
      "linear\n",
      "\u0013of\u0005\n",
      "iteration\n",
      "variable\n",
      "simplex\n",
      "needed\n",
      "metric\n",
      "later\n",
      "without\n",
      "propagate\n",
      "longer\n",
      "deptiro\n",
      "dependence\n",
      "immediately\n",
      "parameter\n",
      "recently\n",
      "perfectly\n",
      "mapping\n",
      "maximum\n",
      "analogue\n",
      "represented\n",
      "number\n",
      "procedure\n",
      "data\n",
      "synchrony\n",
      "tree\n",
      "shared\n",
      "continuous\n",
      "markov\n",
      "see\n",
      "bayesian\n",
      "using\n",
      "posterior\n",
      "timefrequency\n",
      "multilabel\n",
      "quantifies\n",
      "junction\n",
      "ak\n",
      "result\n",
      "grammar\n",
      "conference\n",
      "remaining\n",
      "size\n",
      "spurious\n",
      "classification\n",
      "particular\n",
      "total\n",
      "bayesian\n",
      "research\n",
      "network\n",
      "ordering\n",
      "heavy\n",
      "london\n",
      "architecture\n",
      "leaf\n",
      "measurement\n",
      "pqnli\n",
      "plugin\n",
      "using\n",
      "countably\n",
      "bayesian\n",
      "efficiency\n",
      "applied\n",
      "fails\n",
      "merging\n",
      "effective\n",
      "performance\n",
      "theshold\n",
      "ad\n",
      "infiniteunlabeleddata\n",
      "pt\n",
      "number\n",
      "thus\n",
      "consisting\n",
      "form\n",
      "used\n",
      "attractor\n",
      "williams\n",
      "particular\n",
      "neither\n",
      "natural\n",
      "ad\n",
      "spline\n",
      "gourley\n",
      "introduce\n",
      "hidden\n",
      "interacting\n",
      "one\n",
      "input\n",
      "simple\n",
      "standard\n",
      "move\n",
      "le\n",
      "differential\n",
      "consistently\n",
      "generate\n",
      "national\n",
      "data\n",
      "dependent\n",
      "induction\n",
      "purpose\n",
      "complete\n",
      "kernel\n",
      "using\n",
      "circumstance\n",
      "event\n",
      "hopping\n",
      "xipy\n",
      "figure\n",
      "tree\n",
      "rule\n",
      "association\n",
      "sample\n",
      "equation\n",
      "epoch\n",
      "local\n",
      "interval\n",
      "y\n",
      "completely\n",
      "model\n",
      "requires\n",
      "domain\n",
      "process\n",
      "conditional\n",
      "cause\n",
      "fk\n",
      "handling\n",
      "table\n",
      "cluster\n",
      "harmony\n",
      "behavior\n",
      "hullermeier\n",
      "symbol\n",
      "add\n",
      "also\n",
      "hkx\n",
      "enhanced\n",
      "form\n",
      "gradient\n",
      "language\n",
      "variable\n",
      "system\n",
      "construct\n",
      "spatial\n",
      "event\n",
      "simplex\n",
      "box\n",
      "random\n",
      "estimate\n",
      "tl\n",
      "symbol\n",
      "investigating\n",
      "parameter\n",
      "one\n",
      "function\n",
      "connected\n",
      "th\n",
      "remaining\n",
      "utilizing\n",
      "network\n",
      "cj\n",
      "effort\n",
      "discrete\n",
      "spiking\n",
      "interesting\n",
      "transfer\n",
      "conf\n",
      "weight\n",
      "data\n",
      "identified\n",
      "sometimes\n",
      "current\n",
      "used\n",
      "differing\n",
      "difficult\n",
      "previous\n",
      "idea\n",
      "also\n",
      "shown\n",
      "exhaustive\n",
      "input\n",
      "sli\n",
      "among\n",
      "training\n",
      "process\n",
      "function\n",
      "experiment\n",
      "need\n",
      "achieved\n",
      "bump\n",
      "kingmans\n",
      "state\n",
      "pairwise\n",
      "april\n",
      "reinforcement\n",
      "data\n",
      "exploiting\n",
      "classification\n",
      "regression\n",
      "variable\n",
      "algorithm\n",
      "gamma\n",
      "concept\n",
      "proposal\n",
      "satisfy\n",
      "model\n",
      "topic\n",
      "rule\n",
      "hmm\n",
      "generated\n",
      "dimensionality\n",
      "reference\n",
      "le\n",
      "knowledgebased\n",
      "gibbs\n",
      "data\n",
      "mean\n",
      "qf\n",
      "employ\n",
      "alignment\n",
      "hierarchical\n",
      "approximation\n",
      "reward\n",
      "consider\n",
      "algorithm\n",
      "variable\n",
      "set\n",
      "series\n",
      "function\n",
      "outofn\n",
      "noting\n",
      "neuroscience\n",
      "hidden\n",
      "infinite\n",
      "normalizing\n",
      "learning\n",
      "gamma\n",
      "experiment\n",
      "start\n",
      "infinite\n",
      "equilibrium\n",
      "id\n",
      "also\n",
      "test\n",
      "nj\n",
      "mechanism\n",
      "regime\n",
      "system\n",
      "neural\n",
      "training\n",
      "ieee\n",
      "hx\n",
      "perform\n",
      "might\n",
      "parameter\n",
      "training\n",
      "first\n",
      "allow\n",
      "learning\n",
      "unit\n",
      "layer\n",
      "therefore\n",
      "model\n",
      "learning\n",
      "description\n",
      "lastlayer\n",
      "ncoalesent\n",
      "test\n",
      "importance\n",
      "term\n",
      "greedyrate\n",
      "process\n",
      "satisfy\n",
      "used\n",
      "consider\n",
      "bayes\n",
      "update\n",
      "parameter\n",
      "control\n",
      "genus\n",
      "verified\n",
      "independent\n",
      "assignment\n",
      "take\n",
      "reported\n",
      "population\n",
      "hamming\n",
      "state\n",
      "within\n",
      "kaufmann\n",
      "standard\n",
      "size\n",
      "particular\n",
      "calculating\n",
      "number\n",
      "july\n",
      "ieee\n",
      "process\n",
      "gibbs\n",
      "snp\n",
      "coalescent\n",
      "generalized\n",
      "several\n",
      "define\n",
      "jensenrenyi\n",
      "resistance\n",
      "theory\n",
      "agglomerative\n",
      "open\n",
      "framework\n",
      "fp\n",
      "variable\n",
      "following\n",
      "parse\n",
      "many\n",
      "information\n",
      "hall\n",
      "av\n",
      "solution\n",
      "mdri\n",
      "computationally\n",
      "bradtke\n",
      "performance\n",
      "network\n",
      "yield\n",
      "micro\n",
      "every\n",
      "doucet\n",
      "ghahramani\n",
      "advance\n",
      "set\n",
      "service\n",
      "dynamic\n",
      "nonetheless\n",
      "number\n",
      "require\n",
      "example\n",
      "composite\n",
      "interestingly\n",
      "inference\n",
      "match\n",
      "coding\n",
      "decision\n",
      "tri\n",
      "extreme\n",
      "parameter\n",
      "feature\n",
      "transition\n",
      "event\n",
      "layer\n",
      "sampled\n",
      "convergence\n",
      "cole\n",
      "size\n",
      "neighbouring\n",
      "value\n",
      "dependent\n",
      "model\n",
      "left\n",
      "incomplete\n",
      "consists\n",
      "multilabel\n",
      "network\n",
      "interpreted\n",
      "right\n",
      "input\n",
      "highlighted\n",
      "state\n",
      "construct\n",
      "fk\n",
      "small\n",
      "classification\n",
      "kabadi\n",
      "\u0014\u0010\n",
      "test\n",
      "data\n",
      "best\n",
      "property\n",
      "neural\n",
      "initial\n",
      "level\n",
      "training\n",
      "involves\n",
      "model\n",
      "time\n",
      "point\n",
      "effect\n",
      "network\n",
      "data\n",
      "curve\n",
      "making\n",
      "perplexity\n",
      "lm\n",
      "learning\n",
      "relation\n",
      "lm\n",
      "girosi\n",
      "phase\n",
      "note\n",
      "capture\n",
      "noise\n",
      "rule\n",
      "scale\n",
      "process\n",
      "particle\n",
      "remaining\n",
      "spellmode\n",
      "point\n",
      "report\n",
      "studied\n",
      "solution\n",
      "critical\n",
      "use\n",
      "algorithm\n",
      "density\n",
      "order\n",
      "pattern\n",
      "dw\n",
      "mental\n",
      "editor\n",
      "provides\n",
      "line\n",
      "backpropagation\n",
      "performance\n",
      "ii\n",
      "nns\n",
      "three\n",
      "thus\n",
      "median\n",
      "kp\n",
      "training\n",
      "term\n",
      "watkins\n",
      "case\n",
      "oxford\n",
      "solution\n",
      "action\n",
      "sf\n",
      "fk\n",
      "eg\n",
      "sampling\n",
      "vector\n",
      "achieved\n",
      "state\n",
      "experimenting\n",
      "particular\n",
      "example\n",
      "equivalent\n",
      "oracle\n",
      "abstract\n",
      "computing\n",
      "psychology\n",
      "indicate\n",
      "duration\n",
      "observation\n",
      "node\n",
      "discus\n",
      "em\n",
      "alignment\n",
      "distribution\n",
      "college\n",
      "asynchronous\n",
      "ssp\n",
      "il\n",
      "solving\n",
      "ignore\n",
      "classification\n",
      "undirected\n",
      "left\n",
      "process\n",
      "tp\n",
      "system\n",
      "metric\n",
      "york\n",
      "sofge\n",
      "fourth\n",
      "used\n",
      "reducing\n",
      "input\n",
      "atomic\n",
      "index\n",
      "extent\n",
      "work\n",
      "network\n",
      "density\n",
      "local\n",
      "symbol\n",
      "vision\n",
      "found\n",
      "optimal\n",
      "true\n",
      "dirichlet\n",
      "respectively\n",
      "system\n",
      "perturbed\n",
      "routing\n",
      "find\n",
      "learning\n",
      "bayes\n",
      "direct\n",
      "harmony\n",
      "optimisation\n",
      "observation\n",
      "fine\n",
      "entry\n",
      "number\n",
      "performance\n",
      "plotted\n",
      "epoch\n",
      "dynamical\n",
      "order\n",
      "domain\n",
      "backpropagation\n",
      "shannon\n",
      "merging\n",
      "topic\n",
      "process\n",
      "solve\n",
      "dynamic\n",
      "det\n",
      "symmetric\n",
      "model\n",
      "chen\n",
      "denker\n",
      "ed\n",
      "distinct\n",
      "rijvj\n",
      "harikrishna\n",
      "relatively\n",
      "multilayer\n",
      "weight\n",
      "naturalistic\n",
      "outperform\n",
      "test\n",
      "metric\n",
      "examined\n",
      "rissanen\n",
      "required\n",
      "one\n",
      "optimization\n",
      "ckk\n",
      "hidden\n",
      "map\n",
      "combination\n",
      "algorithm\n",
      "ezt\n",
      "refja\n",
      "field\n",
      "hidden\n",
      "different\n",
      "data\n",
      "using\n",
      "way\n",
      "williams\n",
      "kernel\n",
      "derthick\n",
      "nl\n",
      "time\n",
      "computed\n",
      "llr\n",
      "pp\n",
      "set\n",
      "demanding\n",
      "task\n",
      "much\n",
      "prior\n",
      "key\n",
      "dependency\n",
      "see\n",
      "figure\n",
      "tree\n",
      "marginal\n",
      "pick\n",
      "section\n",
      "document\n",
      "time\n",
      "centred\n",
      "network\n",
      "gammamj\n",
      "additional\n",
      "model\n",
      "conference\n",
      "note\n",
      "updated\n",
      "weight\n",
      "define\n",
      "factor\n",
      "sot\n",
      "ask\n",
      "left\n",
      "work\n",
      "applied\n",
      "vector\n",
      "several\n",
      "tree\n",
      "em\n",
      "kunapuli\n",
      "observe\n",
      "px\n",
      "tj\n",
      "queue\n",
      "experimental\n",
      "tight\n",
      "using\n",
      "space\n",
      "sum\n",
      "posterior\n",
      "recognition\n",
      "right\n",
      "soo\n",
      "add\n",
      "many\n",
      "proportion\n",
      "technical\n",
      "move\n",
      "pattern\n",
      "hidden\n",
      "ie\n",
      "\u0018\u0006\u000f\u0007\n",
      "symbol\n",
      "magnitude\n",
      "inference\n",
      "state\n",
      "slavic\n",
      "sol\n",
      "map\n",
      "variable\n",
      "reinforcement\n",
      "representation\n",
      "example\n",
      "observation\n",
      "wi\n",
      "different\n",
      "time\n",
      "definition\n",
      "model\n",
      "given\n",
      "nevertheless\n",
      "dp\n",
      "marginal\n",
      "pda\n",
      "dembczynski\n",
      "equilibrium\n",
      "signal\n",
      "network\n",
      "one\n",
      "estimate\n",
      "search\n",
      "class\n",
      "related\n",
      "user\n",
      "tlo\n",
      "journal\n",
      "conditional\n",
      "issue\n",
      "bengio\n",
      "smolensky\n",
      "density\n",
      "via\n",
      "symbol\n",
      "new\n",
      "succinctly\n",
      "information\n",
      "recurrent\n",
      "ww\n",
      "becomes\n",
      "variable\n",
      "fixed\n",
      "xt\n",
      "dashed\n",
      "algorithm\n",
      "many\n",
      "derivative\n",
      "conversely\n",
      "high\n",
      "encourage\n",
      "xi\n",
      "data\n",
      "since\n",
      "signal\n",
      "et\n",
      "parameter\n",
      "second\n",
      "point\n",
      "spellmode\n",
      "model\n",
      "system\n",
      "image\n",
      "row\n",
      "linear\n",
      "equation\n",
      "known\n",
      "written\n",
      "tree\n",
      "estimation\n",
      "algorithm\n",
      "isolated\n",
      "learning\n",
      "pt\n",
      "relaxation\n",
      "variance\n",
      "function\n",
      "still\n",
      "point\n",
      "figure\n",
      "give\n",
      "neither\n",
      "afosr\n",
      "epoch\n",
      "hofever\n",
      "parameter\n",
      "word\n",
      "greedyrate\n",
      "adaptive\n",
      "ieeetac\n",
      "matrix\n",
      "experiment\n",
      "string\n",
      "et\n",
      "new\n",
      "given\n",
      "tjiz\n",
      "setup\n",
      "operation\n",
      "gaussian\n",
      "comprise\n",
      "density\n",
      "testing\n",
      "three\n",
      "figure\n",
      "statistic\n",
      "contrast\n",
      "metric\n",
      "closely\n",
      "pz\n",
      "according\n",
      "synchrony\n",
      "realworld\n",
      "mhproposal\n",
      "giles\n",
      "derive\n",
      "behaviour\n",
      "distribution\n",
      "local\n",
      "fpit\n",
      "set\n",
      "character\n",
      "capturing\n",
      "kingmans\n",
      "satisfied\n",
      "figure\n",
      "net\n",
      "call\n",
      "fourth\n",
      "hanson\n",
      "also\n",
      "studied\n",
      "pqnli\n",
      "instance\n",
      "wu\n",
      "region\n",
      "thanks\n",
      "shown\n",
      "parentxi\n",
      "spurious\n",
      "state\n",
      "reject\n",
      "experimental\n",
      "inference\n",
      "top\n",
      "data\n",
      "proposed\n",
      "jr\n",
      "nonetheless\n",
      "consistent\n",
      "consistency\n",
      "zdi\n",
      "network\n",
      "algorithm\n",
      "traditional\n",
      "ranking\n",
      "japan\n",
      "sampling\n",
      "based\n",
      "model\n",
      "interested\n",
      "simple\n",
      "hu\n",
      "probability\n",
      "value\n",
      "dependent\n",
      "sampling\n",
      "mapping\n",
      "ck\n",
      "size\n",
      "definition\n",
      "role\n",
      "node\n",
      "particle\n",
      "shown\n",
      "state\n",
      "plugin\n",
      "jf\n",
      "pt\n",
      "lineage\n",
      "iteration\n",
      "section\n",
      "different\n",
      "matrix\n",
      "update\n",
      "phonetic\n",
      "distribution\n",
      "signal\n",
      "science\n",
      "word\n",
      "regression\n",
      "representation\n",
      "trial\n",
      "ith\n",
      "measure\n",
      "estimate\n",
      "performance\n",
      "inversion\n",
      "conditioning\n",
      "college\n",
      "dependent\n",
      "neighbor\n",
      "category\n",
      "mr\n",
      "model\n",
      "multilabel\n",
      "training\n",
      "extending\n",
      "network\n",
      "proposed\n",
      "removed\n",
      "cn\n",
      "markov\n",
      "property\n",
      "layer\n",
      "tree\n",
      "training\n",
      "using\n",
      "procedure\n",
      "novel\n",
      "number\n",
      "dataset\n",
      "system\n",
      "value\n",
      "yxi\n",
      "furthermore\n",
      "industrial\n",
      "labelling\n",
      "sampling\n",
      "node\n",
      "statistical\n",
      "behavior\n",
      "process\n",
      "theorem\n",
      "xt\n",
      "let\n",
      "let\n",
      "iii\n",
      "procedure\n",
      "ie\n",
      "centered\n",
      "u\n",
      "perturbative\n",
      "work\n",
      "series\n",
      "graphical\n",
      "network\n",
      "hyperparameters\n",
      "layer\n",
      "new\n",
      "mn\n",
      "gaussian\n",
      "effective\n",
      "one\n",
      "many\n",
      "data\n",
      "legendre\n",
      "model\n",
      "density\n",
      "system\n",
      "determined\n",
      "well\n",
      "prior\n",
      "included\n",
      "task\n",
      "marginally\n",
      "connected\n",
      "al\n",
      "proposal\n",
      "neural\n",
      "daniel\n",
      "use\n",
      "ti\n",
      "likelihood\n",
      "indicator\n",
      "proc\n",
      "network\n",
      "neural\n",
      "architecture\n",
      "standard\n",
      "coolen\n",
      "computation\n",
      "neighborhood\n",
      "split\n",
      "simple\n",
      "setting\n",
      "gr\n",
      "iptl\n",
      "sri\n",
      "bad\n",
      "cell\n",
      "dealing\n",
      "energy\n",
      "sinwet\n",
      "define\n",
      "advance\n",
      "new\n",
      "network\n",
      "tli\n",
      "reference\n"
     ]
    }
   ],
   "source": [
    "# Generate doc\n",
    "theta_d = ed.Dirichlet(alpha)\n",
    "for i in range(100):\n",
    "    z_dn = ed.Categorical(probs=theta_d)\n",
    "    phi_z = \n",
    "    w_dn = ed.Categorical(probs=beta[z_dn])\n",
    "    print(idx_to_word[w_dn.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6f7e4ee9b84b8d90e260fc0e260ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before [[1948.2192  1948.2751 ]\n",
      " [1251.2192  1251.2751 ]\n",
      " [ 796.21924  796.27515]]\n",
      "after [[1558.9817 1524.9785]\n",
      " [1031.4255 1035.2314]\n",
      " [ 641.7817  640.5525]]\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.7681685 , 0.82452303], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.8167327 , 0.87247795], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.8638188, 0.9190715], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.90961564, 0.96447587], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.95430493, 1.0088501 ], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([0.99804085, 1.0523303 ], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.0409529, 1.0950313], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.0831494, 1.137053 ], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.1247246, 1.1784807], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.1657573, 1.219388 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# MAX_IT = 10\n",
    "# EPS = 0.001\n",
    "# tf.executing_eagerly()\n",
    "# optim = tf.keras.optimizers.Adam(1e-3)\n",
    "# # B = ed.Dirichlet(concentration=tf.fill([K, V], 0.1), name=\"topics\")\n",
    "# # Z = ed.DirichletMultinomial(tf.convert_to_tensor(Ns), concentration=tf.fill([D, K], 0.1))\n",
    "# alpha = np.copy(alpha_n).astype(np.float32)\n",
    "# eta = np.copy(eta_n).astype(np.float32)\n",
    "\n",
    "# beta = np.copy(beta_n).astype(np.float32)\n",
    "# phi = [np.full((n, K), 1/K).astype(np.float32) for n in Ns]\n",
    "# gamma = np.copy(gamma_n).astype(np.float32)\n",
    "# lmbd = np.copy(lmbd_n).astype(np.float32)\n",
    "\n",
    "# bb = None\n",
    "# gg = None\n",
    "# ww = None\n",
    "\n",
    "# class Positive(tf.keras.constraints.Constraint):\n",
    "#     def __call__(self, w):\n",
    "#         return w * tf.cast(tf.math.greater(w, 0.), w.dtype)\n",
    "\n",
    "# bb = []\n",
    "# for it in trange(MAX_IT):\n",
    "#     bb2 = []\n",
    "#     gg2 = []\n",
    "#     ww2 = []\n",
    "#     print('before', gamma)\n",
    "#     for d in range(D):\n",
    "#         for n in range(Ns[d]):\n",
    "#             for i in range(K):\n",
    "#                 phi[d][n, i] = beta[i, data_enc.iloc[d][n]] * np.exp(digamma(gamma[d, i]) - digamma(np.sum(gamma[d])))\n",
    "#         phi[d] /= np.sum(phi[d], axis=-1, keepdims=True) + 1e-5\n",
    "        \n",
    "#         for i in range(K):\n",
    "#             gamma[d, i] = alpha[i] + np.sum(phi[d][:, i])\n",
    "#     print('after', gamma)\n",
    "\n",
    "#     lmbd = np.full((K, V), eta)\n",
    "#     for i in range(K):\n",
    "#         for j in range(V):\n",
    "#             for d in range(D):\n",
    "#                 mask = (data_enc.iloc[d] == j)\n",
    "#                 lmbd[i, j] += np.sum(phi[d][:, i]*mask)\n",
    "    \n",
    "     \n",
    "# #     if bb is None:\n",
    "# #         bb = bb2\n",
    "# #         gg = gg2\n",
    "# #         ww = ww2\n",
    "# #         break\n",
    "                \n",
    "#     alpha_t = tf.Variable(alpha, trainable=True, constraint=Positive())\n",
    "#     gamma_t = tf.convert_to_tensor(gamma, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "#     def f_x():\n",
    "#         g_term = tf.math.reduce_sum(tf.expand_dims((alpha_t - 1), 0)*(tf.math.digamma(gamma_t) - \n",
    "#                                                    tf.math.digamma(tf.math.reduce_sum(gamma_t, axis=1, keepdims=True))), axis=1)\n",
    "#         loss = -tf.math.reduce_sum(tf.math.lgamma(tf.math.reduce_sum(alpha_t)) - tf.math.reduce_sum(tf.math.lgamma(alpha_t)) + g_term)\n",
    "#         return loss\n",
    "    \n",
    "#     for itt in range(10):\n",
    "#         for i in range(K):\n",
    "#             for itt1 in range(50):\n",
    "#                 #with tf.GradientTape() as tape:\n",
    "#                 optim.minimize(f_x, [alpha_t])\n",
    "# #                 grads = tape.gradient(loss, opt_a)\n",
    "# #                 optim.apply_gradients([(grads, opt_a)])\n",
    "#                 alpha[i] = alpha_t.numpy()[i]\n",
    "#                 np.nan_to_num(alpha, copy=False, nan=1e-5)\n",
    "#                 alpha_t.assign(alpha)\n",
    "#         print(alpha_t)\n",
    "#     beta = (lmbd - eta) / (np.sum(lmbd - eta, axis=-1, keepdims=True) + 1e-5)\n",
    "#     break\n",
    "    \n",
    "#     eta_t = tf.Variable(eta, trainable=True, constraint=Positive())\n",
    "    \n",
    "#     @tf.function\n",
    "#     def f_eta():\n",
    "#         loss = K*((eta_t-1)*(tf.math.digamma(eta_t) - tf.math.digamma(eta_t*V)) + tf.math.lgamma(eta_t*V) - V*tf.math.lgamma(eta))\n",
    "#         return loss\n",
    "    \n",
    "#     for itt1 in range(50):\n",
    "#         optim.minimize(f_eta, [eta_t])\n",
    "#     eta = eta_t.numpy()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (edward)",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
